
<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Banco de Quest√µes ‚Äî v3.1</title>
<link rel="stylesheet" href="styles_v3.css"/>
</head>
<body>
<div class="container">
  <div class="header">
    <div class="brand">üèÅ Banco de Quest√µes ‚Äî <span class="version">v3.1</span></div>
    <div class="toolbar">
      <button class="btn" id="homeBtn">üè† Home</button>
      <button class="btn" id="restartBtn">üîÑ Reiniciar modo</button>
      <span class="badge timer" id="timer">‚è± 00:00</span>
      <span class="badge progress-text" id="progressText"></span>
    </div>
  </div>
  <div class="progress"><div id="progressBar"></div></div>
  <div id="view" class="card"></div>
</div>

<div class="modal-backdrop" id="modal"><div class="modal"><h3>Instru√ß√µes</h3><ul><li>Interface em <b>PT-BR</b>; enunciados em <b>ingl√™s</b> (originais do documento).</li><li><b>Modos</b>: Aprendizagem (ordem do documento), Aprendizagem Aleat√≥rio (ordem aleat√≥ria) e Dif√≠cil (ordem aleat√≥ria, s√≥ avan√ßa se acertar).</li><li><b>Alternativas</b> embaralhadas em todos os modos; a numera√ß√£o da quest√£o (Q#) √© a original.</li><li>Quest√µes <i>multi‚Äëresposta</i> (‚ÄúSelect two/three/four‚Äù): marque a quantidade correta e clique <b>Confirmar</b>.</li><li>No Dif√≠cil, ao errar: mostramos a correta e o menu <b>Tentar novamente</b> ou <b>Ir para Modo Aprendizagem</b>.</li><li>Ao finalizar: acertos/percentual/tempo, com op√ß√£o de <b>Modo Revis√£o</b> (apenas as erradas) ‚Äî revis√µes em sequ√™ncia at√© zerar.</li><li>Sem salvamento de progresso: reabrir a p√°gina reinicia tudo.</li></ul><div class="footer-actions"><button class="btn primary" id="closeModal">Entendi</button></div></div></div>
<div class="confetti" id="confetti"></div>

<script>
const DATA = {questions: [{"qnum": 1, "prompt": "Which NVIDIA solution is specif ically designed to accelerate da ta analytics and machine  learning workloads, \nallowing data scientists to build a nd deploy models at scale us ing GPUs?", "options": ["NVIDIA CUDA", "NVIDIA JetPack", "NVIDIA RAPIDS", "NVIDIA DGX A100"], "correct": [2], "multi": false, "required": 1}, {"qnum": 2, "prompt": "Your team is running an AI infer ence workload on a Kubernetes c luster with multiple NVIDIA GPUs. \nYou observe that some nodes with GPUs are underutilized, while others are overloaded, leading to \ninconsistent inference performan ce across the cluster. Which st rategy would most effectively balance \nthe GPU workload across the Kubernetes cluster?", "options": ["Deploying a GPU-aware scheduler in Kubernetes", "Implementing GPU resource quotas to limit GPU usage per pod", "Using CPU-based autoscali ng to balance the workload", "Reducing the number of G PU nodes in the cluster"], "correct": [0], "multi": false, "required": 1}, {"qnum": 3, "prompt": "A large enterprise is deploying a  high-performance AI infrastru cture to accelerate its machine \nlearning workflows. They are using multiple NVIDIA GPUs in a di stributed environment. To optimize \nthe workload distribution and max imize GPU utilization, which of the following tools or frameworks \nshould be integrated into their system? (Select two)", "options": ["NVIDIA CUDA", "NVIDIA NGC (NVIDIA GPU Cloud)", "TensorFlow Serving", "NVIDIA NCCL (NVIDIA Coll ective Communications Library)", "Keras"], "correct": [0, 3], "multi": true, "required": 2}, {"qnum": 4, "prompt": "You are managing the deployment  of an AI-driven security system  that needs to process video \nstreams from thousands of cameras across multiple locations in real time. The system must \ndetectpotential threats and send a lerts with minimal latency. W hich NVIDIA solution would be most \nappropriate to handle this large-s cale video analytics workload ?", "options": ["NVIDIA Clara Guardian", "NVIDIA Jetson Nano", "NVIDIA DeepStream", "NVIDIA RAPIDS"], "correct": [2], "multi": false, "required": 1}, {"qnum": 5, "prompt": "A data center is running a clus ter of NVIDIA GPUs to support va rious AI workloads. The operations \nteam needs to monitor GPU performance to ensure workloads are r unning efficiently and to prevent \npotential hardware failures. Which two key measures should they  focus on to monitor the GPUs \neffectively? (Select two)", "options": ["Disk I/O rates", "CPU clock speed", "GPU temperature and power consumption", "GPU memory utilization", "Network bandwidth usage"], "correct": [2, 3], "multi": true, "required": 2}, {"qnum": 6, "prompt": "You have developed two different  machine learning models to predict house prices based on various \nfeatures like location, size, a nd number of bedrooms. Model A u ses a linear regression approach, \nwhile Model B uses a random fo rest algorithm. You need to compa re the performance of these \nmodels to determine which one is  better for depl oyment. Which two statistical performance metrics \nwould be most appropriate to compare the accuracy and reliabili ty of these models? (Select two)", "options": ["F1 Score", "Learning Rate", "Mean Absolute Error (MAE)", "Cross-Entropy Loss", "R-squared (Coefficien t of Determination)"], "correct": [2, 4], "multi": true, "required": 2}, {"qnum": 7, "prompt": "Your AI data center is running multiple high-performance GPU workloads, and you notice that certain servers are being underutilized wh ile others are consistently a t full capacity, leading to inefficiencies. \nWhich of the following strategi es would be most effective in ba lancing the workload across your AI \ndata center?", "options": ["Use horizontal scaling to add more servers", "Manually reassign workloads  based on current utilization", "Implement NVIDIA GPU Operator  with Kubernetes for automatic resource scheduling", "Increase cooling cap acity in the data center"], "correct": [2], "multi": false, "required": 1}, {"qnum": 8, "prompt": "What is a key consideration when virtualizing accelerated infra structure to support AI workloads on a \nhypervisor-based environment?", "options": ["Enable vCPU pinni ng to specific cores", "Disable GPU overcommitment in the hypervisor", "Maximize the number of VMs per physical server", "Ensure GPU passthrough i s configured correctly"], "correct": [3], "multi": false, "required": 1}, {"qnum": 9, "prompt": "You are responsible for  scaling an AI infras tructure that proce sses real-time data using multiple \nNVIDIA GPUs. During peak usage,  you notice significant delays in data processing times, even though \nthe GPU utilization is below 80% . What is the most likely cause  of this bottleneck?", "options": ["High CPU usage causing bottlen ecks in data preprocessing", "Overprovisioning of GPU res ources, leading to idle times", "Insufficient memory bandwidth on the GPUs", "Inefficient data transfer  between nodes in the cluster"], "correct": [3], "multi": false, "required": 1}, {"qnum": 10, "prompt": "A financial institution is using an NVIDIA DGX SuperPOD to trai n a large-scale AI model for real-time \nfraud detection. The model requi res low-latency processing and high-throughput data management. \nDuring the training phase, the team  notices significant delays in data processing, causing the GPUs to \nidle frequently. The system is c onfigured with NVMe storage, and the data pipeline involves DALI \n(Data Loading Library) and RAP IDS for preprocessing. Which of t he following actions is most likely to \nreduce data processing delays a nd improve GPU utilization?", "options": ["Switch from NVMe to traditio nal HDD storage for better relia bility", "Disable RAPIDS and use a CPU-based data processing approach", "Optimize the data pipeline wi th DALI to reduce preprocessing  latency", "Increase the number of NVMe storage devices"], "correct": [2], "multi": false, "required": 1}, {"qnum": 11, "prompt": "In a large-scale AI training envi ronment, a data scientist need s to schedule multiple AI model training \njobs with varying dependencies and priorities. Which orchestrat ion strategy would be most effective \nto ensure optimal resource uti lization and job execution order?", "options": ["DAG-Based Workflow Orchestration", "Manual Scheduling", "FIFO (First-In-First-Out) Queue", "Round-Robin Scheduling"], "correct": [0], "multi": false, "required": 1}, {"qnum": 12, "prompt": "You are optimizing an AI data cente r that uses NVIDIA GPUs for energy efficiency. Which of the \nfollowing practices would most eff ectively reduce energy consum ption while maintaining \nperformance?", "options": ["Disabling power capping to allow full power usage", "Enabling NVIDIA's Adaptive Power Management features", "Running all GPUs at maximum clock speeds", "Utilizing older GPUs to reduce power consumption"], "correct": [1], "multi": false, "required": 1}, {"qnum": 13, "prompt": "Which NVIDIA software component is primarily used to manage and  deploy AI models in production \nenvironments, providing support for multiple frameworks and ensuring efficient inference?", "options": ["NVIDIA Triton Inference Server", "NVIDIA TensorRT", "NVIDIA NGC Catalog", "NVIDIA CUDA Toolkit"], "correct": [0], "multi": false, "required": 1}, {"qnum": 14, "prompt": "Which of the following networking features is most critical when designing an AI environment to \nhandle large-scale deep learning model training?", "options": ["Implementing network segment ation to isolate different parts  of the AI environment", "Using Wi-Fi for flexibilit y in connecting compute nodes", "High network throughput with l ow latency between compute nod es", "Enabling network redundancy to prevent single points of fail ure"], "correct": [2], "multi": false, "required": 1}, {"qnum": 15, "prompt": "Your AI team is deploying a large -scale inference service that must process real-time data 24. Given \nthe high availability requirement s and the need to minimize energy consumption, which approach \nwould best balance these objectives?", "options": ["Implement an auto-scaling gr oup of GPUs that adjusts the num ber of active GPUs based on the \nworkload", "Use a GPU cluster with a fix ed number of GPUs always running  at 50% capacity to save energy", "Schedule inference tasks to run in batches during off-peak h ours", "Use a single powerful GPU that operates continuously at full  capacity to handle all inference tasks"], "correct": [0], "multi": false, "required": 1}, {"qnum": 16, "prompt": "You are managing an AI project fo r a healthcare a pplication tha t processes large vo lumes of medical \nimaging data using deep learning models. The project requires h igh throughput and low latency \nduring inference. The deployment environment is an on-premises data center equipped with NVIDIA \nGPUs. You need to select the most appropriate sof tware stack to  optimize the AI workload \nperformance while ensuring scala bility and ease of  management. Which of the following software \nsolutions would be the best ch oice to deploy your deep learning  models?", "options": ["NVIDIA TensorRT", "Docker", "Apache MXNet", "NVIDIA Nsight Systems"], "correct": [0], "multi": false, "required": 1}, {"qnum": 17, "prompt": "You are managing an AI cluster where multiple jobs with varying resource demands are scheduled. Some jobs require exclusive GPU access, while others can share GPUs. Which of the following job \nscheduling strategies would best optimize GPU resource utilizat ion across the cluster?", "options": ["Schedule all jobs wit h dedicated GPU resources", "Use FIFO (First In, First Out) Scheduling", "Enable GPU sharing and use NVI DIA GPU Operator with Kubernet es", "Increase the default pod res ource requests in Kubernetes"], "correct": [2], "multi": false, "required": 1}, {"qnum": 18, "prompt": "You are managing an AI-driven aut onomous vehicle project that r equires real-time decision-making \nand rapid processing of large data  volumes from sensors like Li DAR, cameras, and radar. The AI \nmodels must run on the vehicle's onboard hardware to ensure low latency and high reliability. Which NVIDIA solutions would be most appr opriate to use in this scena rio? (Select two)", "options": ["NVIDIA DGX A100", "NVIDIA Jetson AGX Xavier", "NVIDIA GeForce RTX 3080", "NVIDIA DRIVE AGX Pegasus", "NVIDIA Tesla T4"], "correct": [1, 3], "multi": true, "required": 2}, {"qnum": 19, "prompt": "An enterprise is deploying a lar ge-scale AI model for real-time  image recognition. They face \nchallenges with scalability a nd need to ensure high availabilit y while minimizing latency. Which \ncombination of NVIDIA technologie s would best address these needs?", "options": ["NVIDIA CUDA and NCCL", "NVIDIA DeepStream and NGC Container Registry", "NVIDIA Triton Inference Server and GPUDirect RDMA", "NVIDIA TensorRT and NVLink"], "correct": [3], "multi": false, "required": 1}, {"qnum": 20, "prompt": "You are managing an AI training wor kload that requires high ava ilability and minimal latency. The \ndata is stored across multiple geographically dispersed data centers, and the compute resources are provided by a mix of on-premises GPUs and cloud-based instances . The model training has been \nexperiencing inconsistent perfor mance, with significant fluctuations in processing time and \nunexpected downtime. Which of the  following strategies is most effective in improving the \nconsistency and reliabilit y of the AI training process?", "options": ["Upgrading to the latest vers ion of GPU drivers on all machin es", "Implementing a hybrid load ba lancer to dynamically distribut e workloads across cloud and onpremises \nresources", "Switching to a single-cloud pr ovider to consolidate all comp ute resources", "Migrating all data to a centr alized data cen ter with high-sp eed networking"], "correct": [1], "multi": false, "required": 1}, {"qnum": 21, "prompt": "Which of the following features of  GPUs is most crucial for acc elerating AI workloa ds, specifically in \nthe context of deep learning?", "options": ["Large amount of onboard cache memory", "Ability to execute parallel  operations across thousands of c ores", "Lower power consumption compared to CPUs", "High clock speed"], "correct": [1], "multi": false, "required": 1}, {"qnum": 22, "prompt": "You are responsible for managi ng an AI infrastructure where mul tiple data scientists are \nsimultaneously running large-scal e training jobs on a shared GP U cluster. One data scientist reports \nthat their training job is running much slower than expected, despite being allocated sufficient GPU resources. Upon investigation, you notice that the storage I/O on the system is consistently high. \nWhat is the most likely cause of the slow performance in the data scientist's training job?", "options": ["Incorrect CUDA version installed", "Inefficient data l oading from storage", "Overcommitted CPU resources", "Insufficient GPU memory allocation"], "correct": [1], "multi": false, "required": 1}, {"qnum": 23, "prompt": "You are deploying a large-scale A I model training pipeline on a  cloud-based infrastructure that uses \nNVIDIA GPUs. During the training, you observe that the system o ccasionally crashes due to memory \noverflows on the GPUs, even though  the overall GPU memory usage  is below the maximum capacity. \nWhat is the most likely cause  of the memory overflows, and what  should youdo to mitigate this \nissue?", "options": ["The model's batch size is too large; reduce the batch size", "The GPUs are not receiving da ta fast enough; increase the data pipeline speed", "The CPUs are overloading the G PUs; allocate more CPU cores t o handle preprocessing", "The system is encountering fragmented memory; enable unified  memory management"], "correct": [3], "multi": false, "required": 1}, {"qnum": 24, "prompt": "In your AI data center, you are r esponsible for deploying and m anaging multiple machine learning \nmodels in production. To streamline this process, you decide to  implement MLOps practices with a \nfocus on job scheduling and orchestration. Which of the followi ng strategies is most aligned with \nachieving reliable and efficient model deployment?", "options": ["Use a CI/CD pipeline to aut omate model training, validation,  and deployment", "Schedule all jobs to run at the same time to maximize GPU ut ilization", "Manually trigger model deployments based on performance metr ics", "Deploy models directly to produ ction without staging environ ments"], "correct": [0], "multi": false, "required": 1}, {"qnum": 25, "prompt": "Which components are essential pa rts of the NVIDIA software sta ck in an AI environment? (Select \ntwo)", "options": ["NVIDIA CUDA Toolkit", "NVIDIA TensorRT", "NVIDIA JetPack SDK", "NVIDIA Nsight Systems", "NVIDIA GameWorks"], "correct": [0, 1], "multi": true, "required": 2}, {"qnum": 26, "prompt": "In your AI infrastructure, several  GPUs have recently failed du ring intensive training sessions. To \nproactively prevent such failures, which GPU metric should you monitor most closely?", "options": ["GPU Temperature", "Power Consumption", "Frame Buffer Utilization", "GPU Driver Version"], "correct": [0], "multi": false, "required": 1}, {"qnum": 27, "prompt": "You are assisting a senior researcher in analyzing the results of several AI model experiments \nconducted with different traini ng datasets and hyperparameter configurations. The goal is to \nunderstand how these variables i nfluence model overfitting and generalization. Which method \nwould best help in identifying t rends and relationships between  dataset characteristics, \nhyperparameters, and the risk of overfitting?", "options": ["Perform a time series analysi s of accuracy across different epochs", "Create a scatter plot com paring training accuracy and valida tion accuracy", "Use a histogram to display the frequency of overfitting occu rrences across datasets", "Conduct a decision tree analys is to explore how dataset char acteristics and hyperparameters \naffect overfitting"], "correct": [3], "multi": false, "required": 1}, {"qnum": 28, "prompt": "In your AI data center, you need to ensure continuous performan ce and reliability across all \noperations. Which two strategies are most critical for effectiv e monitoring? (Select two)", "options": ["Conducting weekly performance re views without real-time moni toring", "Using manual logs to track system performance daily", "Disabling non-essential mon itoring to reduce system overhead", "Deploying a comprehensive monitoring system that includes re al-time metrics on CPU, GPU, and \nmemory usage", "Implementing predictive maintenance based on historical hard ware performance data"], "correct": [3, 4], "multi": true, "required": 2}, {"qnum": 29, "prompt": "In an AI environment, the NVIDIA software stack plays a crucial role in ensuring seamless operations \nacross different stages of the A I workflow. Which components of  the NVIDIA software stack would \nyou use to accelerate AI model tr aining and deployment? (Select  two)", "options": ["NVIDIA cuDNN (CUDA Dee p Neural Network library)", "NVIDIA TensorRT", "NVIDIA DGX-1", "NVIDIA Nsight", "NVIDIA DeepStream SDK"], "correct": [0, 1], "multi": true, "required": 2}, {"qnum": 30, "prompt": "You are managing an AI infrastructure using NVIDIA GPUs to trai n large language models for a social \nmedia company. During training, you observe that the GPU utiliz ation is significantly lower than \nexpected, leading to longer tra ining times. Which of the following actions is most likely to improve \nGPU utilization and reduce training time?", "options": ["Use mixed precision training", "Decrease the model complexity", "Increase the batc h size during training", "Reduce the learning rate"], "correct": [0], "multi": false, "required": 1}, {"qnum": 31, "prompt": "When virtualizing an infrastruct ure that includes GPUs to support AI workloads, what is one critical \nfactor to consider to ensure optimal performance?", "options": ["Use GPU sharing technologies , like NVIDIA GRID, to allocate resources dynamically", "Assign more storage to each virtual machine", "Increase the number of virt ual CPUs assigned to each VM", "Disable hyper-thread ing on the host machine"], "correct": [0], "multi": false, "required": 1}, {"qnum": 32, "prompt": "You are tasked with deploying a mach ine learning model into a p roduction environment for real-time \nfraud detection in financial transactions. The model needs to c ontinuously learn from new data and \nadapt to emerging patterns of f raudulent behavior. Which of the following approaches should you \nimplement to ensure the model's accuracy and relevance over tim e?", "options": ["Use a static dataset to r etrain the model periodically", "Deploy the model once and ret rain it only when accuracy drop s significantly", "Continuously retrain the mode l using a streaming data pipeli ne", "Run the model in parallel with  rule-based systems to ensure redundancy"], "correct": [2], "multi": false, "required": 1}, {"qnum": 33, "prompt": "An AI operations team is tasked w ith monitoring a large-scale A I infrastructure where multiple GPUs \nare utilized in parallel. To ensure optimal performance and ear ly detection of issues, which two \ncriteria are essential for mon itoring the GPUs? (Select two)", "options": ["GPU utilization percentage", "Number of active CPU threads", "Average CPU temperature", "Memory bandwidth usage on GPUs", "GPU fan noise levels"], "correct": [0, 3], "multi": true, "required": 2}, {"qnum": 34, "prompt": "You are tasked with optimizing an AI-driven financial modeling application that performs both \ncomplex mathematical calculatio ns and real-time data analytics.  The calculations are CPU-intensive, \nrequiring precise sequential proc essing, while the data analytics involves processing large datasets in \nparallel. How should you allocate the  workloads across GPU and CPU architectures?", "options": ["Use CPUs for data analytics and GPUs for mathe matical calcul ations", "Use GPUs for mathematical cal culations and CPUs for managing  I/O operations", "Use CPUs for mathematical calculations and GPUs for data ana lytics", "Use GPUs for both the mathematical calculations and data ana lytics"], "correct": [2], "multi": false, "required": 1}, {"qnum": 35, "prompt": "In a distributed AI training environment, you notice that the GPU utilization drops significantly when the model reaches the backpropagation stage, leading to increas ed training time. What is the most \neffective way to address this issue?", "options": ["Increase the number of layers in the model to create more wo rk for the GPUs during \nbackpropagation", "Increase the learning rate to  speed up the training process", "Optimize the data loading pipe line to ensure continuous GPU data feeding during backpropagation", "Implement mixed-precision tra ining to reduce the computation al load during backpropagation"], "correct": [3], "multi": false, "required": 1}, {"qnum": 36, "prompt": "You are working on deploying a deep learning model that require s significant GPU resources across \nmultiple nodes. You need to ensure that the model training is s calable, with efficient data transfer \nbetween the nodes to minimize latency. Which of the following n etworking technologies is most \nsuitable for this scenario?", "options": ["Wi-Fi 6", "Fiber Channel", "InfiniBand", "Ethernet (1 Gbps)"], "correct": [2], "multi": false, "required": 1}, {"qnum": 37, "prompt": "Which of the following NVIDIA com pute platforms is best suited for deploying AI workloads at the \nedge with minimal latency?", "options": ["NVIDIA GRID", "NVIDIA Tesla", "NVIDIA RTX", "NVIDIA Jetson"], "correct": [3], "multi": false, "required": 1}, {"qnum": 38, "prompt": "Your AI team is using Kubernetes t o orchestrate a cluster of NV IDIA GPUs for deep learning training \njobs. Occasionally, some high-prio rity jobs experience delays b ecause lower-priority jobs are \nconsuming GPU resources. Which of  the following actions would most effectively ensu re that highpriority \njobs are allocated GPU resources first?", "options": ["Increase the number of  GPUs in the cluster", "Configure Kubernetes pod priority and preemption", "Manually assign GPUs to high-priority jobs", "Use Kubernetes node affinit y to bind jobs to specific nodes"], "correct": [1], "multi": false, "required": 1}, {"qnum": 39, "prompt": "A company is using a multi-GPU s erver for training a deep learn ing model. The training process is \nextremely slow, and after inves tigation, it is found that the G PUs are not being utilized efficiently. The \nsystem uses NVLink, and the softw are stack includes CUDA, cuDNN , and NCCL. Which of the \nfollowing actions is most likely to improve GPU utilization and  overall training performance?", "options": ["Optimize the model's code to use mixed-precision training", "Disable NVLink and use PCIe f or inter-GPU communication", "Update the CUDA version to the latest release", "Increase the batch size"], "correct": [3], "multi": false, "required": 1}, {"qnum": 40, "prompt": "Which industry has experienced the most profound transformation  due to NVIDIA's AI infrastructure, \nparticularly in reducing product  design cycles and enabling more accurate predictivesimul-ations?", "options": ["Automotive, by accelerating the development of autonomous ve hicles and enhancing safety", "Finance, by enabling real-tim e fraud detection and improving  market predictions", "Manufacturing, by automating quality control and improving s upply chain logistics", "Retail, by improving inventory management and enhancing pers onalized shopping experiences"], "correct": [0], "multi": false, "required": 1}, {"qnum": 41, "prompt": "Your company is developing an AI application that  requires seam less integration of data processing, \nmodel training, and deployment in a  cloud-based environment. Th e application must support realtime \ninference and monitoring of model  performance. Which combinatio n of NVIDIA software \ncomponents is best suited for thi s end-to-end AI development and deployment process?", "options": ["NVIDIA DeepOps + NVIDIA RAPIDS", "NVIDIA Clara Deploy SDK + NV IDIA Triton Inference Server", "NVIDIA RAPIDS + NVIDIA TensorRT", "NVIDIA RAPIDS + NVIDIA Triton Inference Server + NVIDIA Deep Ops"], "correct": [3], "multi": false, "required": 1}, {"qnum": 42, "prompt": "During routine monitoring of your AI data center, you notice th at several GPU nodes are consistently \nreporting high memory usage but low  compute usage. What is the most likely cause of this situation?", "options": ["The power supply to the G PU nodes is insufficient", "The GPU drivers are outdated and need updating", "The workloads are being run w ith models that are too small f or the available GPUs", "The data being processed include s large datasets that are st ored in GPU memory but not \nefficiently utilized by the compute cores"], "correct": [3], "multi": false, "required": 1}, {"qnum": 43, "prompt": "You are tasked with contributing to the operations of an AI dat a center that require s high availability \nand minimal downtime. Which str ategy would most effectively hel p maintain continuous AI \noperations in collaboration with t he data center administrator?", "options": ["Implement a failover system w here DPUs manage the AI model inference during GPU downtime", "Deploy a redundant set of CPUs to take over GPU workloads in case of failure", "Use GPUs in active-passive c lusters, with DPUs handling real-time network failover and security", "Schedule regular maintenance  during peak hours to ensure tha t GPUs and DPUs are always \noperational"], "correct": [2], "multi": false, "required": 1}, {"qnum": 44, "prompt": "You are assisting a senior data scientist in optimizing a distr ibuted training pipeline for a deep \nlearning model. The model is being trained across multiple NVID IA GPUs, but the tra ining process is \nslower than expected. Your task is to analyze the data pipeline  and identify potenti al bottlenecks. \nWhich of the following is the most likely cause of the slower-t han-expected train ing performance?", "options": ["The data is not being sh arded across GPUs properly", "The batch size is set too high f or the GPUs' memory capacity", "The learning rate is too low", "The model's architecture is too complex"], "correct": [0], "multi": false, "required": 1}, {"qnum": 45, "prompt": "A healthcare company is using NVI DIA AI infrastructure to develop a deep learning model that can \nanalyze medical images and detect anomalies. The team has notic ed that the model performs well \nduring training but fails to generalize when tested on new, unseen dat \na. Which of the following actio ns is most likely to improve the  model's generalization?", "options": ["Reduce the number of training epochs", "Increase the batc h size during training", "Apply data augmentation techniques", "Use a more complex neur al network architecture"], "correct": [2], "multi": false, "required": 1}, {"qnum": 46, "prompt": "A tech startup is building a hi gh-performance AI application that requires processing large datasets \nand performing complex matrix operations. The team is debating whether to use GPUs or CPUs to \nachieve the best performance. What is the most compelling reaso n to choose GPUs over CPUs for \nthis specific use case?", "options": ["GPUs have larger memory caches than CPUs, which speeds up data retrieval for AI processing", "GPUs excel at parallel proces sing, which is ideal for handli ng large datasets and performing \ncomplex matrix operations", "GPUs have higher single-thread  performance, which is crucial  for AI tasks", "GPUs consume less power than C PUs, making them more energy-e fficient for AI tasks"], "correct": [1], "multi": false, "required": 1}, {"qnum": 47, "prompt": "Your team is tasked with accelerating a large-scale deep learni ng training job that involves processing \na vast amount of data with compl ex matrix operations. The curre nt setup uses high-performance \nCPUs, but the training time is still significant. Which architectural feature of GPUs makes them more \nsuitable than CPUs for this task?", "options": ["Low power consumption", "High core clock speed", "Massive parallelism with thousands of cores", "Large cache memory"], "correct": [2], "multi": false, "required": 1}, {"qnum": 48, "prompt": "As a junior team member, you are  tasked with running data analy sis on a large dataset using NVIDIA \nRAPIDS under the supervision of a senior engineer. The senior engineer advises you to ensure that \nthe GPU resources are effectiv ely utilized to speed up the data  processing tasks. What is the best \napproach to ensure efficient us e of GPU resources during your d ata analysis tasks?", "options": ["Disable GPU acceleration t o avoid potential compatibility is sues", "Use CPU-based pandas for  all DataFrame operations", "Focus on using only CPU cores for parallel processing", "Use cuDF to accelera te DataFrame operations"], "correct": [3], "multi": false, "required": 1}, {"qnum": 49, "prompt": "A data center is designed to suppor t large-scale AI training an d inference workloads using a \ncombination of GPUs, DPUs, and CPUs. During peak workloads, the system begins to experience \nbottlenecks. Which of the followi ng scenarios most effectively uses GPUs and DPUs to resolve the \nissue?", "options": ["Redistribute computational ta sks from GPUs to DPUs to balance the workload evenly between \nboth", "Use DPUs to take over the pr ocessing of certain AI models, allowing GPUs to focus s olely on highpriority \ntasks", "Offload network, storage, and security management from the C PU to the DPU, freeing up the CPU \nand GPU to focus on AI computation", "Transfer memory management fr om GPUs to DPUs to reduce the l oad on GPUs during peak times"], "correct": [2], "multi": false, "required": 1}, {"qnum": 50, "prompt": "Which NVIDIA hardware and software combination is best suited f or training large-scale deep \nlearning models in a data center environment?", "options": ["NVIDIA Quadro GPUs with RAPIDS for real-time analytics", "NVIDIA DGX Station with CUDA t oolkit for model deployment", "NVIDIA A100 Tensor Core GPUs with PyTorch and CUDA for model  training", "NVIDIA Jetson Nano with TensorRT for training"], "correct": [2], "multi": false, "required": 1}, {"qnum": 51, "prompt": "You are tasked with managing an A I training environment where m ultiple deep learning models are \nbeing trained simultaneously on a  shared GPU cluster. Some models require more GPU resources and \nlonger training times than others . Which orchestration strategy  would best ensure that all models are \ntrained efficiently without causi ng delays for high-priority wo rkloads?", "options": ["Implement a priority-based s cheduling system that allocates more GPUs to high-priority models.", "Use a first-come, first-served (FCFS) scheduling policy for all models.", "Randomly assign GPU resources to each model training job.", "Assign equal GPU resources to a ll models regardless of their  requirements."], "correct": [0], "multi": false, "required": 1}, {"qnum": 52, "prompt": "Which of the following statements  best explains why AI workload s are more effectively handled by \ndistributed computing environments?", "options": ["Distributed computing environm ents allow parallel processing  of AI tasks, speeding up training \nand inference.", "AI models are inherently sim pler, making them well-suited to  distributed environments.", "Distributed systems reduce th e need for specialized hardware  like GPUs.", "AI workloads require less me mory than traditional workloads,  which is best managed by \ndistributed systems."], "correct": [0], "multi": false, "required": 1}, {"qnum": 53, "prompt": "Your company is running a distribu ted AI application that invol ves real-time data ingestion from IoT \ndevices spread across multiple locations. The AI model processi ng this data requi res high throughput \nand low latency to deliver actiona ble insights in near real-tim e. Recently, the appl ication has been \nexperiencing intermittent delays and data loss, leading to decr eased accuracy in the AI model's \npredictions. Which action would be st improve the performance an d reliability of the AI application in \nthis scenario?", "options": ["Implementing a dedicated, high- bandwidth network link betwee n IoT devices and the data \nprocessing system.", "Switching to a batch processi ng model to reduce the frequency of data transfers.", "Deploying a Content Delivery N etwork (CDN) to cache data closer to the IoT devices.", "Upgrading the IoT devices to more powerful hardware."], "correct": [0], "multi": false, "required": 1}, {"qnum": 54, "prompt": "In an AI-focused data center , ensuring high data throughput is critical for feeding large datasets to \ntraining models efficiently. Whi ch strategy would best optimize  data throughput in this environment?", "options": ["Use a RAID 5 configuration t o increase redundancy and throug hput.", "Implement NVMe SSDs for faster data access and higher throughput.", "Use traditional HDD storage sy stems due to their high storag e capacity.", "Implement a distributed file s ystem without considering the underlying hardware."], "correct": [1], "multi": false, "required": 1}, {"qnum": 55, "prompt": "You are part of a team analyzing the results of a machine learn ing experiment that involved training \nmodels with different hyperparamet er settings acr oss various da tasets. The goal is to identify trends \nin how hyperparameters and dataset characteristics influence mo del performance, particularly \naccuracy and overfitting. Which analysis method would best help  in identifying the  relationships \nbetween hyperparameters, dataset characteristics, and model per formance?", "options": ["Conduct a correlation matri x analysis between hyperparameter s, dataset characteristics, and \nperformance metrics.", "Apply PCA (Principal Component Analysis) to reduce the dimen sionality of hyperparameter \nsettings.", "Create a bar chart compari ng accuracy for different hyperpar ameter settings.", "Use a pie chart to show the  distribution of accuracy scores across datasets."], "correct": [0], "multi": false, "required": 1}, {"qnum": 56, "prompt": "You are part of a team working on op timizing an AI model that p rocesses video data in real-time. The \nmodel is deployed on a system with multiple NVIDIA GPUs, and th e inference speed is not meeting \nthe required thresholds. You have been tasked with analyzing the data processing p ipeline under the \nguidance of a senior engineer. W hich action would most likely i mprove the inference speed of the \nmodel on the NVIDIA GPUs?", "options": ["Enable CUDA Unified Memory for the model.", "Disable GPU power-saving features.", "Profile the data loading process to ensure it's not a bottle neck.", "Increase the batch siz e used during inference."], "correct": [2], "multi": false, "required": 1}, {"qnum": 57, "prompt": "You are managing an AI data center where energy consumption has  become a critical concern due to \nrising costs and sustainability goa ls. The data center supports various AI workloads, including model \ntraining, inference, and data prep rocessing. Which strategy would most effectively reduce energy \nconsumption without significantly impacting performance?", "options": ["Consolidate all AI workloads onto a single GPU to reduce ove rall power usage.", "Implement dynamic voltage and frequency scaling (DVFS) to ad just GPU power usage based on \nworkload demands.", "Schedule all AI workloads during nighttime to take advantage  of lower electricity rates.", "Reduce the clock speed of all GPUs to lower power consumptio n."], "correct": [1], "multi": false, "required": 1}, {"qnum": 58, "prompt": "Your AI model training proce ss suddenly slows down, and upon in spection, you notice that some of \nthe GPUs in your multi-GPU setup are operating at full capacity  while others are barely being used. \nWhat is the most likely cause of this imbalance?", "options": ["The AI model code is optim ized only for specific GPUs.", "Different GPU models are used in the same setup.", "GPUs are not properly installe d in the server chassis.", "Data loading process is not eve nly distributed across GPUs."], "correct": [3], "multi": false, "required": 1}, {"qnum": 59, "prompt": "You are working on a high-performan ce AI workload that requires  the deployment of deep learning \nmodels on a multi-GPU cluster. The workload needs to scale acro ss multiple nodes efficiently while \nmaintaining high throughput and low latency. However, during the deployment, you notice that the GPU utilization is uneven across the nodes, leading to performa nce bottlenecks. Which of the \nfollowing strategies would be the m ost effective in addressing the uneven GPU utilization in this \nmulti-node AI deployment?", "options": ["Use a CPU-based load bala ncer to distribute tasks.", "Enable GPU affinity in the job scheduler.", "Increase the batch s ize of the workload.", "Enable mixed precision training."], "correct": [1], "multi": false, "required": 1}, {"qnum": 60, "prompt": "An autonomous vehicle company is de veloping a self-driving car that must detect and classify objects \nsuch as pedestrians, other vehicles, and traffic signs in real- time. The system needs to make splitsecond \ndecisions based on complex visual dat \na. Which approach should the c ompany prioritize to effectively address this challenge?", "options": ["Develop an unsupervised learnin g algorithm to cluster visual data and classify objects based on \nsimilarity.", "Apply a linear regression model to predict the position of o bjects based on camera inputs.", "Use a rule-based AI system to classify objects based on pred efined visual characteristics.", "Implement a deep learning model with convolutional neural ne tworks (CNNs) to process and \nclassify visual data."], "correct": [3], "multi": false, "required": 1}, {"qnum": 61, "prompt": "During a high-intensity AI tr aining session on your  NVIDIA GPU cluster, you notice a sudden drop in \nperformance. Suspecting thermal throttling, which GPU monitorin g metric should you prioritize to \nconfirm this issue?", "options": ["Memory Bandwidth Utilization", "CPU Utilization", "GPU Temperature and Thermal Status", "GPU Clock Speed"], "correct": [2], "multi": false, "required": 1}, {"qnum": 62, "prompt": "In an MLOps pipeline, you are responsible for managing the trai ning and deployment of machine \nlearning models on a multi-node G PU cluster. The data used for training is updated frequently. How \nshould you design your job scheduling process to ensure models are trained on the most recent data \nwithout causing unnecessary delays in deployment?", "options": ["Schedule the entire pipeline to run at fixed intervals, rega rdless of data updates.", "Train models only once per w eek and deploy them immediately after training.", "Implement an event-driven sch eduling system that triggers th e pipeline whenever new data is \navailable.", "Use a round-robin scheduling poli cy across all pipeline stag es, regardless of data freshness."], "correct": [2], "multi": false, "required": 1}, {"qnum": 63, "prompt": "Which of the following best desc ribes how memory and storage re quirements differ between \ntraining and inference in AI systems?", "options": ["Training and inference have i dentical memory and storage req uirements since both involve \nprocessing data with the same models.", "Training generally requires more memory and storage due to t he need to process large datasets \nand store intermediate gradients.", "Inference usually requires more memory than training because  of the need to load multiple \nmodels simultaneously.", "Training can be done with minimal memory, focusing more on GPU performance, while inference requires extensive storage."], "correct": [1], "multi": false, "required": 1}, {"qnum": 64, "prompt": "You are managing a high-performance AI cluster where multiple d eep learning jobs are scheduled to \nrun concurrently. To maximize re source efficiency, which of the  following strategies should youuse to \nallocate GPU resources across the cluster?", "options": ["Use a priority queue to assign GPUs to jobs based on their d eadline, ensuring the most timesensitive \njobs complete first.", "Allocate all GPUs t o the largest job to ensure its rapid com pletion, then proceed with smaller jobs.", "Allocate GPUs to jobs based on their compute intensity, rese rving the most powerful GPUs for the \nmost demanding tasks.", "Assign jobs to GPUs based on the ir geographic proximity to r educe data transfer times."], "correct": [2], "multi": false, "required": 1}, {"qnum": 65, "prompt": "When extracting insights from large datasets using data mining and data visualization techniques, \nwhich of the following practices is most critical to ensure acc urate and actionable results?", "options": ["Using complex algorithms with the highest computational cost .", "Ensuring the data is cleane d and pre-processed appropriately .", "Visualizing all possible data  points in a single chart.", "Maximizing the size of the da taset used for training models."], "correct": [1], "multi": false, "required": 1}, {"qnum": 66, "prompt": "A healthcare company is traini ng a large convolutional neural n etwork (CNN) for medical image \nanalysis. The dataset is enorm ous, and training is taking longe r than expected. The team needs to \nspeed up the training process by distributing the workload acro ss multiple GPUs and nodes. Which of \nthe following NVIDIA solutions will help them achieve optimal p erformance?", "options": ["NVIDIA cuDNN", "NVIDIA NCCL and NVIDIA DALI", "NVIDIA DeepStream SDK", "NVIDIA TensorRT"], "correct": [1], "multi": false, "required": 1}, {"qnum": 67, "prompt": "In a complex AI-driven autonomous  vehicle system, the computing  infrastructure is composed of \nmultiple GPUs, CPUs, and DPUs. During real-time object detectio n, which of the following best \nexplains how these components in teract to optimize performance?", "options": ["The GPU processes object d etection algorithms, the CPU handles decision-making logic, and the \nDPU offloads network and storage tasks.", "The CPU processes the object detection model, while the GPU and DPU handle data preprocessing \nand network traffic.", "The GPU processes the object detection model, the DPU offloa ds network traffic from the GPU, \nand the CPU is unused.", "The GPU handles object detect ion algorithms, while the CPU m anages the vehicle's control \nsystems without DPU involvement."], "correct": [0], "multi": false, "required": 1}, {"qnum": 68, "prompt": "When virtualizing a GPU-accelerate d infrastructure, which of th e following is a critical consideration \nto ensure optimal performance for AI workloads?", "options": ["Ensuring proper NUMA (Non-Uniform Memory Access) alignment", "Maximizing the number of VMs per GPU", "Allocating more virtual CPUs (vCPUs) than physical CPUs", "Using software-based GPU virtualization instead of hardware passthrough"], "correct": [0], "multi": false, "required": 1}, {"qnum": 69, "prompt": "Your AI development team is working on a project that involves processing large datasets and \ntraining multiple deep learning models. These models need to be  optimized for deployment on \ndifferent hardware platforms, i ncluding GPUs, CPUs, and edge de vices. Which NVIDIA software \ncomponent would best facilitate the optimization and deployment  of these models across different \nplatforms?", "options": ["NVIDIA TensorRT", "NVIDIA DIGITS", "NVIDIA Triton Inference Server", "NVIDIA RAPIDS"], "correct": [0], "multi": false, "required": 1}, {"qnum": 70, "prompt": "Your AI team notices that the training jobs on yo ur NVIDIA GPU cluster are taking longer than \nexpected. Upon investigation, you s uspect underutilization of t he GPUs. Which monitoring metric is \nthe most critical to determine if the GPUs are being underutilized?", "options": ["GPU Utilization Percentage", "Memory Bandwidth Utilization", "Network Latency", "CPU Utilization"], "correct": [0], "multi": false, "required": 1}, {"qnum": 71, "prompt": "You are working on a project that in volves analyzing a large da taset of satellite images to detect \ndeforestation. The dataset is  too large to be processed on a si ngle machine, so you need to distribute \nthe workload across multiple GPU nodes in a high-performance co mputing cluster. The goal is to use \nimage segmentation techniques to accurately identify deforested  areas. Which approach would be \nmost effective in processing this large dataset of satellite im ages for deforestation detection?", "options": ["Implementing a distributed GPU-accelerated Convolutional Neu ral Network (CNN) for image \nsegmentation", "Storing the images in a tradi tional relational database for easy access and querying", "Using a CPU-based image pro cessing library to preprocess the images before segmentation", "Manually reviewing the images and marking deforested areas f or analysis"], "correct": [0], "multi": false, "required": 1}, {"qnum": 72, "prompt": "What is the primary advantage  of using virtualized environments  for AI workloads in a large \nenterprise setting?", "options": ["Reduces the need for specia lized hardware by running AI workloads on general-purpose CPUs", "Allows for easier scaling of AI workloads across multiple physical machines", "Ensures that AI workloads a re always running on the same phy sical machine for consistency", "Enables AI workloads to util ize cloud resources without requ iring any changes to the underlying \ncode"], "correct": [1], "multi": false, "required": 1}, {"qnum": 73, "prompt": "Your team is tasked with deployi ng a new AI-driven application that needs to perform real-time \nvideo processing and analytics on high-resolution video streams . The application must analyze \nmultiple video feeds simultaneously to detect and classify obje cts with minimal latency. Considering \nthe processing demands, which hardwa re architecture would be th e most suitable for this scenario?", "options": ["Deploy CPUs exclusively for  all video processing tasks", "Deploy GPUs to handle the vi deo processing and analytics", "Use CPUs for video analytics  and GPUs for managing network traffic", "Deploy a combination of CPUs and FPGAs for video processing"], "correct": [1], "multi": false, "required": 1}, {"qnum": 74, "prompt": "A healthcare provider is deploying an AI-driven diagnostic syst em that analyzes medical images to \ndetect diseases. The system must operate with high accuracy and speed to support doctors in realtime. During deployment, it was observed that the system's performanc e degrades when processing \nhigh-resolution images in real-time, leading to delays and occa sional misdiagnoses. What should be \nthe primary focus to improve the  system's real-time processing capabilities?", "options": ["Increase the system's memory to store more images concurrent ly", "Optimize the AI model's archi tecture for better parallel pro cessing on GPUs", "Use a CPU-based system for im age processing to reduce the lo ad on GPUs", "Lower the resolution of input images to reduce the processin g load"], "correct": [1], "multi": false, "required": 1}, {"qnum": 75, "prompt": "You are helping a senior engine er analyze the results of a hype rparameter tuning process for a \nmachine learning model. The resul ts include a large number of trials, each with different \nhyperparameters and corresponding performance metrics. The engi neer asks you to create \nvisualizations that will he lp in understanding how different hy perparameters impact model \nperformance. Which type of visual ization would be most appropri ate for identifying the relationship \nbetween hyperparameters and model performance?", "options": ["Scatter plot of hyperparameter values against performance me trics", "Parallel coordinates plot showing hyperparameters and perfor mance metrics", "Pie chart showing the proportion of successful trials", "Line chart showing perfor mance metrics over trials"], "correct": [1], "multi": false, "required": 1}, {"qnum": 76, "prompt": "Which of the following software components is most responsible for optimizing deep learning \noperations on NVIDIA GPUs by provi ding highly tuned implementat ions of standard routines?", "options": ["CUDA", "TensorFlow", "cuDNN", "NCCL"], "correct": [2], "multi": false, "required": 1}, {"qnum": 77, "prompt": "You are managing an AI infrastructure that includes multiple NV IDIA GPUs across  various virtual \nmachines (VMs) in a cloud envir onment. One of the VMs is consis tently underperforming compared \nto others, even though it has the same GPU allocation and is ru nning similar workloads.What is the \nmost likely cause of the underper formance in this virtual machi ne?", "options": ["Misconfigured GPU passthrough settings", "Inadequate storage I/O performance", "Insufficient CPU allocation for the VM", "Incorrect GPU driver version installed"], "correct": [0], "multi": false, "required": 1}, {"qnum": 78, "prompt": "You are tasked with creating a r eal-time dashboard for monitori ng the performance of a large-scale \nAI system processing social media dat a. The dashboard should provide i nsights into trends, anomalies , and performance metrics using \nNVIDIA GPUs for data processi ng and visualization. Which tool o r technique would most effectively \nleverage the GPU resources to vis ualize real-time insights from  this high-volume social media data?", "options": ["Relying solely on a relational  database to handle the data a nd generate visualizations.", "Implementing a GPU-accelerated deep learning model to genera te insights and f eeding results \ninto a CPU-based visualization tool.", "Using a standard CPU-based ETL (Extract, Transform, Load) pr ocess to prepare the data for \nvisualization.", "Employing a GPU-accelerated tim e-series database for real-ti me data ingestion and visualization."], "correct": [3], "multi": false, "required": 1}, {"qnum": 79, "prompt": "Your AI-driven data center experi ences occasional GPU failures,  leading to significant downtime for \ncritical AI applications. To pr event future issues, you decide to implement a comprehensive GPU \nhealth monitoring system. You nee d to determine which metrics a re essential for predicting and \npreventing GPU failures. Which of  the following metrics should be prioritized to predict potential \nGPU failures and maintain GPU health?", "options": ["GPU Clock Speed", "GPU Temperature", "CPU Utilization", "Error Rates (e.g., ECC errors)"], "correct": [3], "multi": false, "required": 1}, {"qnum": 80, "prompt": "Your organization is planning t o deploy an AI solution that inv olves large-scale data processing, \ntraining, and real-time inference in a cloud environment. The solution must ensure seamless \nintegration of data pipelines, m odel training, and deployment. Which combination of NVIDIA \nsoftware components will best support the entire lifecycle of t his AI solution?", "options": ["NVIDIA RAPIDS + NVI DIA Triton Inference Server + NVIDIA NGC Catalog", "NVIDIA TensorRT + NVIDIA DeepStream SDK", "NVIDIA Triton Inference Se rver + NVIDIA NGC Catalog", "NVIDIA RAPIDS + NVIDIA TensorRT"], "correct": [0], "multi": false, "required": 1}, {"qnum": 81, "prompt": "Which NVIDIA software component is specifically designed to acc elerate the end-to-end data science \nworkflow by leveraging GPU acceleration?", "options": ["NVIDIA CUDA Toolkit", "NVIDIA DeepStream SDK", "NVIDIA TensorRT", "NVIDIA RAPIDS"], "correct": [3], "multi": false, "required": 1}, {"qnum": 82, "prompt": "You are responsible for managi ng an AI data center that handles  large-scale deep learning workloads. \nThe performance of your training j obs has recently degraded, an d you've noticed that the GPUs are \nunderutilized while CPU usage re mains high. Which of the follow ing actions would most likely \nresolve this issue?", "options": ["Increase the GPU memory allocation.", "Reduce the batch size during training.", "Add more GPUs to the system.", "Optimize the data pipelin e for better I/O throughput."], "correct": [3], "multi": false, "required": 1}, {"qnum": 83, "prompt": "You manage a large-scale AI infr astructure where several AI wor kloads are executed concurrently \nacross multiple NVIDIA GPUs. Rec ently, you observe that certain  GPUs are underutilized while others \nare overburdened, leading to subop timal performance and extende d processing times. Which of the \nfollowing strategies is most eff ective in resolving this imbala nce?", "options": ["Implementing dynamic GPU load balancing across the infrastru cture", "Reducing the batch size for all AI workloads", "Increasing the power limit on underutilized GPUs", "Disabling GPU overclocking to normalize performance"], "correct": [0], "multi": false, "required": 1}, {"qnum": 84, "prompt": "You are responsible for managi ng an AI infrastructure that runs  a critical deep lea rning application. \nThe application experiences inter mittent performance drops, esp ecially when processing large \ndatasets. Upon investigation, you find that some of the GPUs ar e not being fully uti lized while others \nare overloaded, causing the overa ll system to underperform. Wha t would be the most effective \nsolution to address the uneven GPU utilization and optimize the  performance of the deep learning \napplication?", "options": ["Reduce the size of the  datasets being processed.", "Increase the clock speed of the GPUs.", "Add more GPUs to the system.", "Implement dynamic load balancing for the GPUs."], "correct": [3], "multi": false, "required": 1}, {"qnum": 85, "prompt": "Your company is planning to deploy  a range of AI workloads, inc luding training a l arge convolutional \nneural network (CNN) for ima ge classification, running real-tim e video analytics, and performing \nbatch processing of sensor dat a. What type of infrastructur e should be prioritized to support  these diverse AI workloads \neffectively?", "options": ["On-premise servers wit h large storage capacity", "CPU-only servers with high memory capacity", "A cloud-based infrastructure with serverless computing optio ns", "A hybrid cloud infrastructure combining on-premise servers a nd cloud resources"], "correct": [3], "multi": false, "required": 1}, {"qnum": 86, "prompt": "You are working with a large healthcare dataset containing mill ions of patient records. Your goal is to \nidentify patterns and extract actionable insights that could im prove patient outcomes. The dataset is \nhighly dimensional, with numerous variables, and requires signi ficant processing power to analyze \neffectively. Which two techniques are most suitable for extract ing meaningful insights from this \nlarge, complex dataset? (Select two)", "options": ["SMOTE (Synthetic Minorit y Over-sampling Technique)", "Data Augmentation", "Batch Normalization", "K-means Clustering", "Dimensionality Reduction (e.g., PCA)"], "correct": [3, 4], "multi": true, "required": 2}, {"qnum": 87, "prompt": "You are working on a project that involves monitoring the perfo rmance of an AI model deployed in \nproduction. The model's accuracy and latency metrics are being tracked over time. Your task, under \nthe guidance of a senior engineer , is to create visualizations that help the team understand trends in \nthese metrics and identify any pot ential issues. Which visualization would be most effective for \nshowing trends in both accuracy and latency metrics over time?", "options": ["Pie chart showing the distribution of accuracy metrics.", "Box plot comparing accuracy and latency.", "Dual-axis line chart with accuracy on one axis and latency o n the other.", "Stacked area chart showing cumulative accuracy and latency."], "correct": [2], "multi": false, "required": 1}, {"qnum": 88, "prompt": "Your organization runs multiple AI workloads on a shared NVIDIA  GPU cluster. Some workloads are \nmore critical than others. Recently, you've noticed that less critical workloads are consuming more \nGPU resources, affecting the performance of critical workloads. What is the best approach to ensure that critical workloads have priority access to GPU resources?", "options": ["Implement GPU Quotas with K ubernetes Resource Management", "Use CPU-based Inference fo r Less Critical Workloads", "Upgrade the GPUs in the Clu ster to More Powerful Models", "Implement Model Optimization Techniques"], "correct": [0], "multi": false, "required": 1}, {"qnum": 89, "prompt": "Which of the following best desc ribes a key difference between training and inference architectures \nin AI deployments?", "options": ["Training requires higher com pute power, while inference prio ritizes low latency and high \nthroughput.", "Inference requires more mem ory bandwidth than training.", "Training architectures prio ritize energy efficiency, while i nference architectures do not.", "Inference architectures require distributed training across multiple GPUs."], "correct": [0], "multi": false, "required": 1}, {"qnum": 90, "prompt": "A financial services company is developing a machine learning m odel to detect fraudulent \ntransactions in real-time. They need to manage the entire AI li fecycle, from data  preprocessing to \nmodel deployment and monitoring. Which combination of NVIDIA so ftware components should they \nintegrate to ensure an efficient and scalable AI development an d deployment process?", "options": ["NVIDIA Clara for model trai ning, TensorRT for data processin g, and Jetson for deployment.", "NVIDIA RAPIDS for data proces sing, TensorRT for model optimi zation, and Triton Inference Server \nfor deployment.", "NVIDIA DeepStream for data  processing, CUDA for model traini ng, and NGC for deployment.", "NVIDIA Metropolis for data c ollection, DIGITS for training, and Triton Inference Server for \ndeployment."], "correct": [1], "multi": false, "required": 1}, {"qnum": 91, "prompt": "You are working with a team of data scientists on an AI project  where multiple machine learning \nmodels are being trained to pred ict customer churn. The models are evaluated based on the Mean \nSquared Error (MSE) as the loss function. However, one model co nsistently shows a higher MSE \ndespite having a more complex arc hitecture compared to simpler models. What is the most likely \nreason for the higher MSE in the more complex model?", "options": ["Low learning rate in model training", "Overfitting to the training data", "Incorrect calculati on of the loss function", "Underfitting due to insufficient model complexity"], "correct": [1], "multi": false, "required": 1}, {"qnum": 92, "prompt": "Which statement correctly differe ntiates between AI, machine le arning, and deep learning?", "options": ["Machine learning is the same  as AI, and deep learning is simply a method within A I that doesn't \ninvolve machine learning.", "AI is a broad field encompassing various technologies, inclu ding machine learning, which focuses \non data-driven models, and deep learning, a subset of machine l earning using neural networks.", "Deep learning is a broader concept than machine learning, which is a specialized form of AI.", "Machine learning is a type of  AI that only uses linear model s, while deep learn ing involves nonlinear \nmodels exclusively."], "correct": [1], "multi": false, "required": 1}, {"qnum": 93, "prompt": "You are planning to deploy a large-scale AI training job in the cloud using NVIDIA GP Us. Which of the \nfollowing factors is most crucia l to optimize both cost and per formance for your deployment?", "options": ["Selecting instances with the  highest available GPU core coun t", "Enabling autoscaling to dynamically allocate resources based  on workload demand", "Ensuring data locality by choos ing cloud regions closest to your data sources", "Using reserved instances ins tead of on-demand instances"], "correct": [1], "multi": false, "required": 1}, {"qnum": 94, "prompt": "Your AI cluster handles a mix of  training and inference workloa ds, each with different GPU resource \nrequirements and runtime prioriti es. What scheduling strategy w ould best optimize th e allocation of \nGPU resources in this mixed-workload environment?", "options": ["Implement FIFO Scheduling Across All Jobs", "Increase the GPU Memo ry Allocation for All Jobs", "Manually Assign GPUs to Jobs Based on Priority", "Use Kubernetes Node Affinit y with Taints and Tolerations"], "correct": [3], "multi": false, "required": 1}, {"qnum": 95, "prompt": "You are tasked with deploying an AI model across multiple cloud  providers, each using NVIDIA GPUs. \nDuring the deployment, you observe that the model's performance  varies significantly between the \nproviders, even though identical instance types and configurations are used. What is the most likely reason for this discrepancy?", "options": ["Variations in cloud provider-specific optimizations and soft ware stack", "Different versions of the AI framework being used across pro viders", "Cloud providers using different c ooling systems for their da ta centers", "Differences in the GPU architecture between the cloud provid ers"], "correct": [0], "multi": false, "required": 1}, {"qnum": 96, "prompt": "Your AI infrastructure team is deploying a large NLP model on a  Kubernetes cluster using NVIDIA \nGPUs. The model inference requir es low latency due to real-time  user interaction. However, the \nteam notices occasional latency spikes. What would be the most effective strategy to mitigate these \nlatency spikes?", "options": ["Deploy the Model on Multi-Inst ance GPU (MIG) Architecture", "Use NVIDIA Triton Inference Server with Dynamic Batching", "Increase the Number of Repli cas in the Kubernetes Cluster", "Reduce the Model Size by Quantization"], "correct": [1], "multi": false, "required": 1}, {"qnum": 97, "prompt": "In your AI data center, you've obs erved that some GPUs are underutilized while othe rs are frequently \nmaxed out, leading to uneven performa nce across workloads. Whic h monitoring tool or technique \nwould be most effective in identifying and resolving these GPU utilization imbalances?", "options": ["Set Up Alerts for Disk I/O Performance Issues", "Perform Manual Daily Checks of GPU Temperatures", "Monitor CPU Utilization Using Standard System Monitoring Too ls", "Use NVIDIA DCGM to Monito r and Report GPU Utilization"], "correct": [3], "multi": false, "required": 1}, {"qnum": 98, "prompt": "You are managing an AI infrastruc ture that supports a healthcare application requiring high \navailability and low latency. The system handles multiple workl oads, including real- time diagnostics, \npatient data analysis, and pred ictive modeling for treatment ou tcomes. To ensure optimal \nperformance, which strategy s hould you adopt for workload distribution and resource management?", "options": ["Prioritize real-time diagnos tics by allocating the majority of resources to these tasks \nanddeprioritize others.", "Manually allocate resources  based on estimated task duration s.", "Implement an auto-scaling str ategy that dynamically adjusts resources based on workload \ndemands.", "Allocate equal resources to all tasks to ensure uniform perf ormance."], "correct": [2], "multi": false, "required": 1}, {"qnum": 99, "prompt": "In an AI data center, ensuring t he health and performance of GPU resources is critical. You notice that \nsome workloads are unexpected ly failing or slowing down. Which monitoring approach would be \nmost effective in proactively detecting and resolving these iss ues?", "options": ["Review system logs weekly.", "Monitor server uptime and network latency.", "Set up NVIDIA DCGM health checks and alerts.", "Deploy automatic workload restart mechanisms."], "correct": [2], "multi": false, "required": 1}, {"qnum": 100, "prompt": "Which of the following statements best differentiates AI, machi ne learning, and deep learning?", "options": ["Machine learning is a type of A I that specifically uses deep  learning algorithms to make \npredictions.", "Deep learning and AI are the same, and machine learning is a  subset of deep learning.", "AI is the broad concept of machines being able to perform ta sks that require human intelligence, \nmachine learning is a subset of  AI, and deep learning is a subs et of machine learning.", "Machine learning is synonymous with AI, and deep learning is  just an alternativ e term for neural \nnetworks."], "correct": [2], "multi": false, "required": 1}, {"qnum": 101, "prompt": "You are assisting in a project whe re the senior engineer requir es you to create visualizations of \nsystem resource usage during the  training of an AI model. The t raining was conducted using multiple \nNVIDIA GPUs over several hours. T he goal is to present the resu lts in a way that highlights periods of \nhigh resource utiliz ation and potential bottlenecks. Which type  of visualization would best illustrate \nperiods of high resource utiliza tion and potential bottlenecks during the training process?", "options": ["Stacked bar chart showing cumulative resource usage.", "Pie chart showing the proportion of time each GPU was utiliz ed.", "Heatmap showing GPU  utilization over time.", "Box plot showing the distribution of resource usage."], "correct": [2], "multi": false, "required": 1}, {"qnum": 102, "prompt": "You are working on an autonomous vehicle project that requires real-time processing of highdefinition \nvideo feeds to detect and respond to objects in the environment. Which NVIDIA solution is \nbest suited for deploying the AI m odels needed for this task in  an embedded system?", "options": ["NVIDIA Mellanox.", "NVIDIA Clara.", "NVIDIA Jetson AGX Xavier.", "NVIDIA BlueField."], "correct": [2], "multi": false, "required": 1}, {"qnum": 103, "prompt": "Your AI data center is running multiple high-power NVIDIA GPUs, and you've noticed an increase in \noperational costs related to power consumption and cooling. Which of the following strategies would \nbe most effective in optimizi ng power and cooling efficiency wi thout compromising GPU \nperformance?", "options": ["Implement AI-based dynamic thermal management systems.", "Reduce GPU utilization by low ering workload intensity.", "Switch to air-cooled GPUs instead of liquid-cooled GPUs.", "Increase the cooling fan speeds of all servers."], "correct": [0], "multi": false, "required": 1}, {"qnum": 104, "prompt": "When setting up a virtualized environment with NVIDIA GPUs, you  notice a signif icant drop in \nperformance compared to running w orkloads on bare metal. Which factor is most likely contributing \nto the performance degradation?", "options": ["Using high-performance networking.", "Overcommitting GPU resources.", "Running VMs on SSD storage.", "Enabling high ava ilability features."], "correct": [1], "multi": false, "required": 1}, {"qnum": 105, "prompt": "A financial institution is implementing a real-time fraud detec tion system using deep learning \nmodels. The system needs to proc ess large volumes of transactio ns with very low latency to identify \nfraudulent activities immediat ely. During testing, the team obs erves that the system occasionally \nmisses fraudulent transactions  under heavy load, and latency sp ikes occur. Which strategy would \nbest improve the system's pe rformance and r eliability?", "options": ["Deploy the model on a CPU cluster instead of GPUs to handle the processing.", "Reduce the complexity of the model to decrease the inference  time.", "Increase the dataset size by including more historical trans action data.", "Implement model parallelism to split the model across multip le GPUs."], "correct": [3], "multi": false, "required": 1}, {"qnum": 106, "prompt": "After deploying an AI model on a n NVIDIA T4 GPU in a production environment, you notice that the \ninference latency is inconsiste nt, varying significantly during  different times of the day. Which of the \nfollowing actions would most likely resolve the issue?", "options": ["Increase the number of inference threads.", "Upgrade the GPU driver.", "Deploy the model on a  CPU instead of a GPU.", "Implement GPU isolation f or the inference process."], "correct": [3], "multi": false, "required": 1}, {"qnum": 107, "prompt": "You are assisting in a project tha t involves deploying a large- scale AI model on a multi-GPU server. \nThe server is experiencing unexp ected performance degradation d uring inference, and you have \nbeen asked to analyze the system under the supervision of a sen ior engineer. Which approach would \nbe most effective in identifying the source of the performance degradation?", "options": ["Monitor the system's power supply levels.", "Check the system's CPU utilization.", "Analyze the GPU memory usage using nvidia-smi.", "Inspect the training data for inconsistencies."], "correct": [2], "multi": false, "required": 1}, {"qnum": 108, "prompt": "In which industry has AI most significantly improved operationa l efficiency through predictive \nmaintenance, leading to reduce d downtime and maintenance costs?", "options": ["Finance", "Retail", "Manufacturing", "Healthcare"], "correct": [2], "multi": false, "required": 1}, {"qnum": 109, "prompt": "You are part of a team analyzing the results of an AI model training process across various hardware \nconfigurations. The objective is  to determine how different hardware factors, such as GPU type, \nmemory size, and CPU-GPU communication speed, affect the model' s training time and final \naccuracy. Which analysis method w ould best help in identifying trends or relati onships between \nhardware factors and model performance?", "options": ["Create a heatmap of  CPU-GPU communication speed versus train ing time.", "Plot a scatter plot of model  performance against GPU type.", "Use a bar chart to compare the average training times across  different hardware configurations.", "Conduct a regression analysis w ith hardware factors as independent variables and model \nperformance metrics as  dependent variables."], "correct": [3], "multi": false, "required": 1}, {"qnum": 110, "prompt": "Your team is deploying an AI mode l that involves a real-time re commendation system for a hightraffic \ne-commerce platform. The model must analyze user behavior and s uggest products instantly \nas the user interacts with the platform. Which type of AI workl oad best describes this use case?", "options": ["Batch processing", "Reinforcement learning", "Streaming analytics", "Offline training"], "correct": [2], "multi": false, "required": 1}, {"qnum": 111, "prompt": "Your AI infrastructure team is managing a deep learning model t raining pipeline that uses NVIDIA \nGPUs. During the model traini ng phase, you observe inconsistent performance, with some GPUs \nunderutilized while others are a t full capacity. What is the mo st effective strategy to optimize GPU \nutilization across th e training cluster?", "options": ["Reconfigure the model to us e mixed precision training.", "Reduce the number of GPUs assigned to the training task.", "Use NVIDIA's Multi-Instance GPU (MIG) feature to partition G PUs.", "Turn off GPU auto-scaling to prevent dynamic resource alloca tion."], "correct": [2], "multi": false, "required": 1}, {"qnum": 112, "prompt": "When implementing an MLOps pipeline, which component is crucial  for managing version control \nand tracking changes in model experiments?", "options": ["Continuous Integration (CI) System", "Model Registry", "Orchestration Platform", "Artifact Repository"], "correct": [1], "multi": false, "required": 1}, {"qnum": 113, "prompt": "You are assisting a senior data scientist in a project aimed at improving the efficiency of a deep learning model. The team is an alyzing how different data prepro cessing techniques impact the \nmodel's accuracy and training time. Your task is to identify wh ich preprocessing techniques have the \nmost significant effect on these metrics. Which method would be most effective in identifying the preprocessing techniques that significantly affect model accura cy and training time?", "options": ["Use a line chart to plot tra ining time for different preproc essing techniques.", "Conduct a t-test between diffe rent preprocessing techniques.", "Perform a multivariate regression analysis with preprocessin g techniques as independent \nvariables and accuracy/training time as dependent variables.", "Create a pie chart showing t he distribution of preprocessing  techniques used."], "correct": [2], "multi": false, "required": 1}, {"qnum": 114, "prompt": "You are tasked with virtualizing t he GPU resources in a multi-t enant AI infrastructure where different \nteams need isolated access to GPU resources. Which approach is most suitable for ensuring efficient \nresource sharing while maintaining isolation between tenants?", "options": ["NVIDIA vGPU (Virtual GPU) Technology", "Using GPU passthrough for each tenant", "Deploying containers w ithout GPU isolation", "Implementing CPU-based virtualization"], "correct": [0], "multi": false, "required": 1}, {"qnum": 115, "prompt": "You are managing a data center running numerous AI workloads on  NVIDIA GPUs. Recently, some of \nthe GPUs have been showing signs of underperformance, leading t o slower job completion times. \nYou suspect that resource uti lization is not opt imal. You need to implement monitoring strategies to \nensure GPUs are effectively utilized and to diagnose any underp erformance. Which of the following \nmetrics is most critical to m onitor for identifying underutiliz ed GPUs in your data center?", "options": ["GPU Core Utilization", "GPU Memory Usage", "Network Bandwidth Utilization", "System Uptime"], "correct": [0], "multi": false, "required": 1}, {"qnum": 116, "prompt": "You are designing a data center plat form for a large-scale AI d eployment that must handle \nunpredictable spikes in demand fo r both training and inference workloads. The goal is to ensure that \nthe platform can scale efficien tly without significant downtime  or performance degradation. Which \nstrategy would best achieve this goal?", "options": ["Deploy a fixed number of high-performance GPU servers with a uto-scaling based on CPU usage.", "Implement a round-robin scheduling policy across all servers  to distribute workloads evenly.", "Migrate all workloads to a single, large cloud instance with  multiple GPUs to handle peak loads.", "Use a hybrid cloud model with on -premises GPUs for steady workloads and cloud GPUs for scaling \nduring demand spikes."], "correct": [3], "multi": false, "required": 1}, {"qnum": 117, "prompt": "You are managing an AI data center where multiple GPUs are orchestrated across a large cluster to run various deep learning tasks. Which of the following actions  best describes an efficient approach \nto cluster orchestration in this environment?", "options": ["Use a round-robin scheduling a lgorithm to distribute jobs ev enly across all GPUs, regardless of \ntheir workload requirements.", "Prioritize job assignments to G PUs with the least power cons umption to reduce energy costs.", "Implement a Kubernetes-based orchestration system to dynamic ally allocate GPU resources based \non workload demands.", "Assign all jobs to the most power ful GPU in the cluster to m aximize performance and minimize job \ncompletion time."], "correct": [2], "multi": false, "required": 1}, {"qnum": 118, "prompt": "You are managing an AI cluster with several nodes, each equipped with multiple NVIDIA GPUs. The cluster supports various machine learning tasks with differing resource requirements. Some jobs are \nGPU-intensive, while others require high memory but minimal GPU usage. Your goal is to efficiently allocate resources to maximize throughput and minimize job wait  times. Which orchestration \nstrategy would best optimize resour ce allocation in this mixed-workload environment?", "options": ["Manually assign jobs to specifi c nodes based on estimated wo rkload requirements.", "Use a dynamic scheduler that ad justs resource allocation based on job requirements and current \ncluster utilization.", "Allocate GPUs evenly across a ll jobs to ensure fair distribu tion.", "Schedule jobs based on a fixe d priority order, regardless of  resource requirements."], "correct": [1], "multi": false, "required": 1}, {"qnum": 119, "prompt": "Your organization is building a hybrid cloud system that needs to handle a variety of tasks, including \ncomplex scientificsimul-ations, database management, and traini ng large AI models. You need to \nallocate resources effectivel y. How do GPU and CPU architecture s compare in terms of handling \nthese different tasks?", "options": ["GPUs are better for parallel tasks like AI model training an dsimul-ations, while CPUs are better for \nsequential tasks like database management.", "CPUs should be used for traini ng AI models, while GPUs are better for database management.", "GPUs should be used exclusively for scientificsimul-ations, and CPUs for everything else.", "GPUs are superior for all type s of workloads in this scenario."], "correct": [0], "multi": false, "required": 1}, {"qnum": 120, "prompt": "Which of the following has been t he most critical factor enabling the recent rapid improvements and \nadoption of AI in various sectors?", "options": ["The rise of user-friendly A I frameworks and libraries.", "The availability of large, a nnotated datasets for training A I models.", "Increased investment in AI research and development by large  tech companies.", "The development and adoption of  AI-specific hardware like GPUs and TPUs."], "correct": [3], "multi": false, "required": 1}, {"qnum": 121, "prompt": "A research team is deploying a deep learning model on an NVIDIA  DGX A100 system. The model has \nhigh computational demands and req uires efficient use of all av ailable GPUs. During the deployment, \nthey notice that the GPUs are  underutilized, and the inter-GPU communication seems to be a \nbottleneck. The software stack includes TensorFlow, CUDA, NCCL, and cuDNN. Which of the following actions would most likely optimize the inter-GPU communication and improve overall GPU \nutilization?", "options": ["Disable cuDNN to streamline GPU operations.", "Increase the number of data parallel jobs running simultaneo usly.", "Ensure NCCL is configured correctly for optimal bandwidth ut ilization.", "Switch to using a single GPU to reduce complexity."], "correct": [2], "multi": false, "required": 1}, {"qnum": 122, "prompt": "When virtualizing a GPU-accelerat ed infrastructure to support A I operations, what is a key factor to \nensure efficient and scalable performance across virtual machin es (VMs)?", "options": ["Increase the CPU a llocation to each VM.", "Ensure that GPU memory is not overcommitted among VMs.", "Allocate more network bandw idth to the host machine.", "Enable nested vir tualization on the VMs."], "correct": [1], "multi": false, "required": 1}, {"qnum": 123, "prompt": "Your company is building an AI-pow ered recommendation engine that will be integrated into an ecommerce \nplatform. The engine will be cont inuously trained on user inter action data using a \ncombination of TensorFlow, PyTorch, and XGBoost models. You nee d a solution that allows you to \nefficiently share datasets acro ss these frameworks, ensuring co mpatibility and high performance on \nNVIDIA GPUs. Which NVIDIA softwa re tool would be most effective  in this situation?", "options": ["NVIDIA cuDNN", "NVIDIA TensorRT", "NVIDIA DALI (Data Loading Library)", "NVIDIA Nsight Compute"], "correct": [2], "multi": false, "required": 1}, {"qnum": 124, "prompt": "You are tasked with optimizing the  performance of a deep learning model used for image \nrecognition. The model needs to pr ocess a large dataset as quic kly as possible while maintaining high \naccuracy. You have access to both GPU and CPU resources. Which two statements best describe why \nGPUs are more suitable than CPU s for this task? (Select two)", "options": ["CPUs are better suited for handling the large dataset due to  their superior memory bandwidth.", "GPUs have a higher number of cores compared to CPUs, allowing for parallel p rocessing of many \noperations simultaneously.", "GPUs are optimized for mat rix operations, which are common i n deep learning algorithms.", "GPUs have a lower latency than CPUs, making them faster for individual calculations.", "CPUs consume less power than GPUs, making them more suitable  for prolonged computations."], "correct": [1, 2], "multi": true, "required": 2}, {"qnum": 125, "prompt": "Your team is building an AI-power ed application t hat requires t he deployment of multiple models, \neach trained using different frame works (e.g., TensorFlow, PyTorch, and ONNX). You need a \ndeployment solution that can effi ciently serve all these models  in production, regardless of the \nframework they were built in. Which software component should y ou choose?", "options": ["NVIDIA Clara Deploy SDK", "NVIDIA TensorRT", "NVIDIA DeepOps", "NVIDIA Triton Inference Server"], "correct": [3], "multi": false, "required": 1}, {"qnum": 126, "prompt": "A global financial instituti on is implementing an AI-driven fraud detection system that must process \nvast amounts of transaction data  in real-time across multiple r egions. The system needs to be highly \nscalable, maintain low latenc y, and ensure data security and co mpliance with various international \nregulations. The infrastructu re should also s upport continuous model updates without disrupting the \nservice. Which combination of NVIDIA technologies would best me et the requirements for this fraud \ndetection system?", "options": ["Implement the system on NVIDIA Quadro GPUs with TensorFlow for model training and deployment.", "Deploy the system on NVIDIA DGX A100 systems with NVIDIA Merlin for real-time data processing \nand model updates.", "Deploy the system on generic CPU-based servers with CUDA for  accelerated computation.", "Use NVIDIA Jetson AGX Xavier de vices for distributed data processing across regional offices."], "correct": [1], "multi": false, "required": 1}, {"qnum": 127, "prompt": "In an AI infrastructure setup us ing NVIDIA GPUs across multiple  nodes, you notice that the internode \ncommunication latency is higher than expected during distribute d training. Which networking \nfeature or protocol is most lik ely responsible for reducing latency in this scenario?", "options": ["Network Address Translation (NAT)", "TCP/IP over Ethernet", "InfiniBand with RDMA (Rem ote Direct Memory Access)", "VLAN segmentation"], "correct": [2], "multi": false, "required": 1}, {"qnum": 128, "prompt": "An AI research team is working on a large-scale natural languag e processing (NLP) model that \nrequires both data preprocessing and training across multiple G PUs. They need to ensure that the \nGPUs are used efficiently to minim ize training time. Which comb ination of NVIDIA technologies \nshould they use?", "options": ["NVIDIA TensorRT  and NVIDIA DGX OS", "NVIDIA DeepStream SDK and NVIDIA CUDA Toolkit", "NVIDIA DALI (Data Loadi ng Library) and NVIDIA NCCL", "NVIDIA cuDNN and NVIDIA NGC Catalog"], "correct": [2], "multi": false, "required": 1}, {"qnum": 129, "prompt": "A financial institution is implementing an AI-driven fraud dete ction system that needs to process \nmillions of transactions daily in real-time. The system must ra pidly identify suspicious activity and \ntrigger alerts, while also con tinuously learning from new data to improve accuracy. Which \narchitecture is most appropriate for this scenario?", "options": ["Single GPU server with local  SSD storage for both training a nd inference", "Edge-only deployment with ARM  processors for both training a nd inference", "Hybrid setup with multi-GPU servers for training and edge de vices for inference", "CPU-based servers with cloud s torage for centralized processing"], "correct": [2], "multi": false, "required": 1}, {"qnum": 130, "prompt": "You have deployed an AI training job on a GPU cluster, but the training time has not decreased as \nexpected after adding more GP Us. Upon further investigation, yo u observe that the GPU utilization is \nlow, and the CPU utilization is very high. What is the most likely cause of this issue?", "options": ["The AI model is not compatible with multi-GPU training.", "The GPUs are not properl y connected in the cluster.", "Incorrect software version installed on the GPUs.", "The data preprocessing is  being bottlenecked by the CPU."], "correct": [3], "multi": false, "required": 1}, {"qnum": 131, "prompt": "When deploying AI workloads on a  cloud platform using NVIDIA GPUs, which of the following is the \nmost critical consideration to e nsure cost efficiency without c ompromising performance?", "options": ["Running all workloads on a singl e, high-performance GPU instance to minimize costs", "Using spot instances where applicable for non-critical workl oads", "Choosing a cloud provider that offers the lowest per-hour GP U cost", "Selecting the instance with the maximum GPU memory available"], "correct": [1], "multi": false, "required": 1}, {"qnum": 132, "prompt": "A financial institution is deploying two different machine lear ning models to predict credit defaults. \nThe models are evaluated using M ean Squared Error (MSE) as the primary metric. Model A has an \nMSE of 0.015, while Model B has an MSE of 0.027. Additionally, the institution is considering the \ncomplexity and interpretability of  the models. Given this infor mation, which model should be \npreferred and why?", "options": ["Model A should be preferred because it has a more complex architecture, leading to better longterm \nperformance.", "Model B should be preferred because it has a higher MSE, ind icating it is less  likely to overfit.", "Model A should be preferred becau se it is more interpretable than Model", "Model A should be preferred because it has a lower MSE, indi cating better performance."], "correct": [3], "multi": false, "required": 1}, {"qnum": 133, "prompt": "A transportation company wants to implement AI to improve the s afety and efficiency of its \nautonomous vehicle fleet. They n eed a solution that can handle real-time data processing, deep \nlearning model inference, and hi gh-throughput workloads. Which NVIDIA solution should they \nconsider deploying?", "options": ["NVIDIA DeepStream", "NVIDIA Clara", "NVIDIA Drive", "NVIDIA Jetson"], "correct": [2], "multi": false, "required": 1}, {"qnum": 134, "prompt": "You are part of a team that is s etting up an AI infrastructure using NVIDIA's DGX systems. The \ninfrastructure is intended to support multiple AI workloads, in cluding training, i nference, and \ndataanalysis. You have been taske d with analyzing system logs t o identify performance bottlenecks \nunder the supervision of a senio r engineer. Which log file woul d be most useful to analyze when \ndiagnosing GPU performance issues in this scenario?", "options": ["Network traffic logs", "NVIDIA GPU utilization logs (nvidia-smi)", "System kernel logs (dmesg)", "Application error logs"], "correct": [1], "multi": false, "required": 1}, {"qnum": 135, "prompt": "You are managing an AI data center platform that runs a mix of compute-intensive training jobs and \nlow-latency inference tasks. Recently, the system has been expe riencing unexpected slowdowns \nduring inference tasks, even though t here are sufficient GPU re sources available. What is the most \nlikely cause of this issue, and how can it be resolved?", "options": ["The inference jobs are running a t the same priority level as the training jobs, causing contention \nfor resources.", "The GPUs are overheating, l eading to thermal  throttling duri ng inference.", "The inference tasks are not opt imized for the GPU architectu re, leading to inefficient use of \nresources.", "The training jobs are consum ing too much network bandwidth, leaving insufficient bandwidth for \ninference data transfer."], "correct": [3], "multi": false, "required": 1}, {"qnum": 136, "prompt": "In a data center designed for AI workloads, what is a key diffe rence in how GPUs and DPUs \ncomplement CPU functionality?", "options": ["GPUs and DPUs are used interchangeably, depending on the spe cific AI workload, without any \nsignificant difference in function.", "GPUs focus on memory management, whereas DPUs focus on accelerating storage throughput for CPUs.", "GPUs enhance floating-point c omputation, while DPUs enhance integer computation, both \ndirectly supporting CPU tasks.", "GPUs are designed for parallel processing of AI models, whil e DPUs manage data center \nnetworking and security tasks to offload CPUs."], "correct": [3], "multi": false, "required": 1}, {"qnum": 137, "prompt": "Which NVIDIA solution is specif ically designed to accelerate th e development and deployment of AI \nin healthcare, particularly in medical imaging and genomics?", "options": ["NVIDIA Jetson", "NVIDIA TensorRT", "NVIDIA Metropolis", "NVIDIA Clara"], "correct": [3], "multi": false, "required": 1}, {"qnum": 138, "prompt": "Which of the following best desc ribes the primary benefit of us ing GPUs over CPUs for AI workloads?", "options": ["GPUs provide better accuracy in AI model predictions.", "GPUs consume less power than CPUs for AI tasks.", "GPUs have higher memory capacity than CPUs.", "GPUs are designed to handle parallel processing tasks effici ently."], "correct": [3], "multi": false, "required": 1}, {"qnum": 139, "prompt": "You are configuring a multi-node AI training environment using NVIDIA GPUs, and y our team wants \nto ensure that the network infra structure can hand le the data t ransfer between nodes efficiently, \nespecially during distributed tra ining tasks. What is the most critical factor to consider in the network \ninfrastructure to minimize bot tlenecks during distributed AI tr aining?", "options": ["Implementing InfiniBand with RDMA support", "Increasing the number of Ethernet ports on each node", "Reducing the number of nodes to simplify the network", "Using software-defined networking (SDN) to manage traffic"], "correct": [0], "multi": false, "required": 1}, {"qnum": 140, "prompt": "During AI model deployment, your t eam notices significant performance degradation in inference \nworkloads. The model is deployed on an NVIDIA GPU cluster with Kubernetes. Which of the following could be the most likel y cause of the degradation?", "options": ["Outdated CUDA drivers", "CPU bottlenecks", "High disk I/O latency", "Insufficient GPU memory allocation"], "correct": [3], "multi": false, "required": 1}, {"qnum": 141, "prompt": "In your multi-tenant AI cluster, multiple workloads are running  concurrently, leading to some jobs \nexperiencing performance degradation. Which GPU monitoring metric is most critical for identifying resource contention between jobs?", "options": ["GPU Utilization Across Jobs", "GPU Temperature", "Network Latency", "Memory Bandwidth Utilization"], "correct": [0], "multi": false, "required": 1}, {"qnum": 142, "prompt": "Your team is tasked with deploying a deep learning model that w as trained on large datasets for \nnatural language processing (NLP). The model will be used in a customer support chatbot, requiring \nfast, real-time responses. Which a rchitectural considerations a re most important when moving from \nthe training environment to the inference environment?", "options": ["Data augmentation and hyperparameter tuning", "Model checkpointing and distributed inference", "Low-latency deployment and scaling", "High memory bandwidth a nd distributed training"], "correct": [2], "multi": false, "required": 1}, {"qnum": 143, "prompt": "Your organization operates an AI cluster where various deep lea rning tasks are executed. Some tasks \nare time-sensitive and must be completed as soon as possible, w hile others are less critical. \nAdditionally, some jobs can be pa rallelized across multiple GPU s, while others cannot. You need to \nimplement a job scheduling policy that balances these needs eff ectively. Which scheduling policy \nwould best balance the needs of time-sensi tive tasks and effici ently utilize the available GPUs?", "options": ["First-Come, First-Served (FC FS) scheduling to maintain order", "Schedule the longest-running j obs first to reduce overall cl uster load", "Use a round-robin scheduling a pproach to ensure equal access  for all jobs", "Implement a priority-based s cheduling system that also consi ders GPU availability and task \nparallelization"], "correct": [3], "multi": false, "required": 1}, {"qnum": 144, "prompt": "A logistics company wants to optimize its delivery routes by pr edicting traffic conditions and delivery \ntimes. The system must process real-time data from various sour ces, such as GPS, weather reports, \nand traffic sensors, to adjust r outes dynamically. Which approa ch should the company use to \neffectively handle this complex scenario?", "options": ["Apply a basic machine learning algorithm, such as decision t rees, to predict delivery times based \non historical data", "Utilize an unsupervised learning a pproach to cluster delivery data and generate  fixed routes", "Use a rule-based AI system to predefine optimal routes based  on historical traffic data", "Implement a deep learning model that uses a convolutional neural network (CNN) to process and predict from multi-source real-time data"], "correct": [3], "multi": false, "required": 1}, {"qnum": 145, "prompt": "Which industry has seen the most significant transformation thr ough the use of NVIDIA AI \ninfrastructure, particularly in enhancing product development c ycles and reducing time-to-market \nfor new innovations?", "options": ["Manufacturing, by automating pr oduction lines and improving quality control", "Retail, by optimizing supply cha ins and enhancing customer p ersonalization", "Finance, by improving predictiv e analytics and algorithmic t rading models", "Automotive, by revolutioni zing the design and testing of autonomous vehicles"], "correct": [3], "multi": false, "required": 1}, {"qnum": 146, "prompt": "Which component of the NVIDIA soft ware stack is primarily respo nsible for optimizing deep learning \nmodels for inference in production environments?", "options": ["NVIDIA DIGITS", "NVIDIA Triton Inference Server", "NVIDIA TensorRT", "NVIDIA CUDA"], "correct": [2], "multi": false, "required": 1}, {"qnum": 147, "prompt": "You are responsible for manag ing an AI-driven fraud detection s ystem that processes transactions in \nreal-time. The system is hoste d on a hybrid cloud  infrastructur e, utilizing both on-premises and \ncloud-based GPU clusters. Recently, the system has been missing  fraud detection alerts due to delays \nin processing data from on-premises servers to the cloud, causi ng significant financial risk to the \norganization. What is the most effective way to reduce latency and ensure timely fraud detection \nacross the hybrid c loud environment?", "options": ["Increasing the number of on-prem ises GPU clusters to handle the workload locally", "Implementing a low-latenc y, high-throughput direct connectio n between the on-premises data \ncenter and the cloud", "Migrating the entire fraud detection workload to on-premises  servers", "Switching to a single-cloud prov ider to centralize all processing in the cloud"], "correct": [1], "multi": false, "required": 1}, {"qnum": 148, "prompt": "Which NVIDIA compute platform is most suitable for large-scale AI training in data c enters, providing \nscalability and flexibility to ha ndle diverse AI workloads?", "options": ["NVIDIA GeForce RTX", "NVIDIA DGX SuperPOD", "NVIDIA Quadro", "NVIDIA Jetson"], "correct": [1], "multi": false, "required": 1}, {"qnum": 149, "prompt": "Which industry has seen the most significant impact from AI-dri ven advancements, particularly in \noptimizing supply chain management and improving customer exper ience?", "options": ["Healthcare", "Education", "Retail", "Real Estate"], "correct": [2], "multi": false, "required": 1}, {"qnum": 150, "prompt": "A company is deploying a large-sc ale AI training workload that requires distribut ed computing across \nmultiple GPUs. They need to ensure efficient communication betw een GPUs on different nodes and \noptimize the training time. Whic h of the following NVIDIA techn ologies should they use to achieve \nthis?", "options": ["NVIDIA NVLink", "NVIDIA TensorRT", "NVIDIA NCCL (NVIDIA Coll ective Communication Library)", "NVIDIA DeepStream SDK"], "correct": [2], "multi": false, "required": 1}, {"qnum": 151, "prompt": "Your AI cluster is managed usi ng Kubernetes with NVIDIA GPUs. D ue to a sudden influx of jobs, your \ncluster experiences resource overc ommitment, where more jobs ar e scheduled than the available \nGPU resources can handle. Which s trategy would most effectively  manage this situation to maintain \ncluster stability?", "options": ["Increase the Maximum Number of Pods per Node", "Schedule Jobs in a Round- Robin Fashion Across Nodes", "Use Kubernetes Horizontal P od Autoscaler Based on Memory Usa ge", "Implement Resource Quotas a nd LimitRanges in Kubernetes"], "correct": [3], "multi": false, "required": 1}, {"qnum": 152, "prompt": "Which component of the AI software  ecosystem is responsible for managing the distribution of deep \nlearning model training across multiple GPUs?", "options": ["NCCL", "cuDNN", "CUDA", "TensorFlow"], "correct": [0], "multi": false, "required": 1}, {"qnum": 153, "prompt": "Your AI team is deploying a rea l-time video processing applicat ion that leverages deep learning \nmodels across a distributed system with multiple GPUs. However, the application faces frequent \nlatency spikes and inconsistent frame processing times, especially when scaling across different \nnodes. Upon review, you find that the network bandwidth between  nodes is becoming a bottleneck, \nleading to these performance i ssues. Which strategy would most effectively reduce latency and \nstabilize frame processing times in this distributed AI applica tion?", "options": ["Increase the number of GPUs per node", "Reduce the video resolu tion to lower the data load", "Optimize the deep learni ng models for lower complexity", "Implement data compression te chniques for inter-node communication"], "correct": [3], "multi": false, "required": 1}, {"qnum": 154, "prompt": "Your AI team is deploying a multi- stage pipeline in a Kubernete s-managed GPU cluster, where some \njobs are dependent on the comple tion of others. What is the most efficient way to ensure that these \njob dependencies are respected du ring scheduling and execution?", "options": ["Increase the Priority of Dependent Jobs", "Use Kubernetes Jobs with Directed Acyclic Graph (DAG) Schedu ling", "Deploy All Jobs Concurrently and Use Pod Anti-Affinity", "Manually Monitor and Trigger Dependent Jobs"], "correct": [1], "multi": false, "required": 1}, {"qnum": 155, "prompt": "A large healthcare provider wants to implement an AI-driven dia gnostic system that can analyze \nmedical images across multiple hospitals. The system needs to h andle large volumes of data, comply \nwith strict data privacy regulations, and provide fast, accurate results. The infrastructure should also support future scaling as more hos pitals join the network. Whic h approach using NVIDIA \ntechnologies would best meet the r equirements for this AI-driven diagnostic system?", "options": ["Deploy the system using generi c CPU servers with TensorFlow for model training and inference", "Implement the AI system on NVI DIA Quadro RTX GPUs across loc al servers in each hospital", "Use NVIDIA Jetson Nano device s at each hospital for image pr ocessing", "Deploy the AI model on NVIDIA DGX A100 systems in a centrali zed data center with NVIDIA Clara"], "correct": [3], "multi": false, "required": 1}, {"qnum": 156, "prompt": "In an AI infrastructure setup, you need to optimize the network  for high-performance data \nmovement between storage syste ms and GPU compute nodes. Which p rotocol would be most \neffective for achieving low latency and high bandwidth in this environment?", "options": ["HTTP", "SMTP", "Remote Direct Memory Access (RDMA)", "TCP/IP"], "correct": [2], "multi": false, "required": 1}, {"qnum": 157, "prompt": "You are comparing several regressi on models that predict the fu ture sales of a product based on \nhistorical dat a. The models vary in complexity a nd computational requirements . Your goal is to select the model \nthat provides the best balance  between accuracy and the ability  to generalize to new data. Which \nperformance metric should you prio ritize to select the most reliable regression model?", "options": ["Mean Squared Error (MSE)", "Accuracy", "R-squared (Coefficien t of Determination)", "Cross-Entropy Loss"], "correct": [2], "multi": false, "required": 1}, {"qnum": 158, "prompt": "Your AI data center is experien cing increased operational costs, and you suspect that inefficient GPU \npower usage is contributing to the  problem. Which GPU monitorin g metric would be most effective \nin assessing and optimizing power efficiency?", "options": ["Performance Per Watt", "Fan Speed", "GPU Memory Usage", "GPU Core Utilization"], "correct": [0], "multi": false, "required": 1}, {"qnum": 159, "prompt": "When designing a data center speci fically for AI workloads, whi ch of the following factors is most \ncritical to optimize for training large-scale neural networks?", "options": ["Maximizing the number of sto rage arrays to handle data volum es", "Deploying the maximum number of CPU cores available in each node", "High-speed, low-latency networking between compute nodes", "Ensuring the data center has a robust virtua lization platfor m"], "correct": [2], "multi": false, "required": 1}, {"qnum": 160, "prompt": "You have completed an analysis  of resource util ization during t he training of a d eep learning model \non an NVIDIA GPU cluster. The sen ior engineer requests that you  create a visualiz ation that clearly \nconveys the relationship betwee n GPU memory usage and model tra ining time across different \ntraining sessions. Which visualiz ation would be most effective in conveying the relationship between \nGPU memory usage and model training time?", "options": ["Bar chart showing average memory usage for each training ses sion", "Histogram of training times", "Line chart showing training time over sessions", "Scatter plot with GPU memor y usage on one axis and training time on the other"], "correct": [3], "multi": false, "required": 1}, {"qnum": 161, "prompt": "You are supporting a senior eng ineer in troubleshooting an AI w orkload that involves real-time data \nprocessing on an NVIDIA GPU cluste r. The system experiences occ asional slowdowns during data \ningestion, affecting the overall pe rformance of the AI model. W hich approach would be most \neffective in diagnosing the cau se of the data ingestion slowdow n?", "options": ["Profile the I/O operations on the storage system", "Switch to a different data preprocessing framework", "Increase the number of GPU s used for data processing", "Optimize the AI model's inference code"], "correct": [0], "multi": false, "required": 1}, {"qnum": 162, "prompt": "You are working under the supervis ion of a senior AI engineer on a project involving large-scale data \nprocessing using NVIDIA GPUs. The t ask involves analyzing a lar ge dataset of images to train a deep \nlearning model. You need to ensure that the data pipeline is op timized for performance while \nminimizing resource usage. Which of the following techniques wo uld best optimize the data pipeline \nfor training a deep learn ing model on NVIDIA GPUs?", "options": ["Load the entire dataset into GPU memory", "Apply data sharding across multiple CPUs", "Use data augmentation on the CPU before sending data to the GPU", "Implement mixed precision training"], "correct": [3], "multi": false, "required": 1}, {"qnum": 163, "prompt": "You are tasked with transforming a traditional da ta center into  an AI-optimized data center using \nNVIDIA DPUs (Data Processing Units). One of your goals is to of fload network and sto rage processing \ntasks from the CPU to the DPU to enhance performance and reduce latency. Which scenario best \nillustrates the advantage of using DPUs in this transformation?", "options": ["Using DPUs to handle network traffic encryption and decrypti on, freeing up CPU resources for AI \nworkloads", "Offloading AI model training tasks from GPUs to DPUs to free up GPU resources for inference", "Using DPUs to process large da tasets in paralle l with CPUs t o speed up data preprocessing for AI", "Offloading GPU memory managemen t tasks to DPUs to improve the efficiency of GPU-based \nworkloads"], "correct": [0], "multi": false, "required": 1}, {"qnum": 164, "prompt": "Your organization is setting up an A I model deployment pipeline  that requires frequent updates. The \nteam needs to ensure minimal downtime during model updates, ver sion control, and monitoring of \nthe models in production. Which sof tware component would be most suitable to handle these \nrequirements?", "options": ["NVIDIA NGC Catalog", "NVIDIA TensorRT", "NVIDIA Triton Inference Server", "NVIDIA DIGITS"], "correct": [2], "multi": false, "required": 1}, {"qnum": 165, "prompt": "Your organization is running a mix ed workload environment that includes both general-purpose \ncomputing tasks (like database management) and specialized task s (like AI model inference). You \nneed to decide between investing in more CPUs or GPUs to optimize performance and costefficiency. How does the architecture of GPUs compare to that of CPUs in th is scenario?", "options": ["GPUs are better suit ed for workloads requiring massive parallelism, while CPUs handle singlethreaded \ntasks more efficiently", "CPUs and GPUs have identical architectures but differ only i n power consumption", "GPUs are optimized for general-purpose computing and can replace CPUs entirely", "CPUs have more cores than GPUs, making them better for all t ypes of workloads"], "correct": [0], "multi": false, "required": 1}, {"qnum": 166, "prompt": "Which NVIDIA solution is specif ically designed for accelerating  and optimizing AI mode l inference in \nproduction environments, particular ly for applications requirin g low latency?", "options": ["NVIDIA TensorRT", "NVIDIA DGX A100", "NVIDIA DeepStream", "NVIDIA Omniverse"], "correct": [0], "multi": false, "required": 1}, {"qnum": 167, "prompt": "You are tasked with designing a high ly available AI data center platform that can continue to operate \nsmoothly even in the event of hard ware failures. The platform m ust support both training and \ninference workloads with minimal  downtime. Which architecture w ould best meet these \nrequirements?", "options": ["Deploy a single, powerful GP U server with redundant power su pplies and network interfaces", "Implement a distributed architecture with multiple GPU serve rs and a load balanc er to distribute \nthe workload", "Set up a warm standby system wh ere another data center mirrors the primary one and is manually \nactivated", "Use a cluster of CPU-based ser vers with RAID storage to ensu re data redundancy and protection"], "correct": [1], "multi": false, "required": 1}, {"qnum": 168, "prompt": "A retail company wants to implement an AI-based system to predi ct customer behavior and \npersonalize product recommendations  across its online platform. The system needs to analyze vast \namounts of customer data, inc luding browsing history, purchase patterns, and social media \ninteractions. Which approach would be the most effective for ac hieving these goals?", "options": ["Utilizing unsupervised learning to automatically classify cu stomers into different categories \nwithout labeled data", "Implementing a rule-based AI system to generate recommendati ons based on predefined \ncustomer criteria", "Using a simple linear regressi on model to predict customer b ehavior based on purchase history \nalone", "Deploying a deep learning model that uses a neural network w ith multiple layers for feature \nextraction and prediction"], "correct": [3], "multi": false, "required": 1}, {"qnum": 169, "prompt": "Your AI data center is experienci ng fluctuating workloads where  some AI models require significant \ncomputational resources at specific times, while others have a steady demand. Which of the \nfollowing resource management strategies would be most effectiv e in ensuring effici ent use of GPU \nresources across varying workloads?", "options": ["Use Round-Robin Scheduling for Workloads", "Implement NVIDIA MIG (Multi-Instance GPU) for Resource Parti tioning", "Manually Schedule Workloads Based on Expected Demand", "Upgrade All GPUs to the Latest Model"], "correct": [1], "multi": false, "required": 1}, {"qnum": 170, "prompt": "You are tasked with deploying a real-time recommendation system  for an e-commerce platform \nusing NVIDIA AI infrastructure. The system needs to process millions of user interactions per second \nto provide personalized recommenda tions instantly. Which NVIDIA solution is best suited to handle \nthis workload efficiently?", "options": ["NVIDIA Clara", "NVIDIA DGX Station", "NVIDIA Triton Inference Server", "NVIDIA TensorRT"], "correct": [2], "multi": false, "required": 1}, {"qnum": 171, "prompt": "You are tasked with deploying multiple AI workloads in a data c enter that supports both virtualized \nand non-virtualized environments. T o maximize resource efficien cy and flexibility, which of the \nfollowing strategies would be most effective for running AI wor kloads in a virtua lized environment?", "options": ["Use containerization within a single VM to run multiple AI w orkloads, leveraging shared resources \nefficiently", "Deploy each AI workload in a s eparate virtual machine (VM) t o isolate resources and prevent \ninterference", "Use a single VM to run all AI w orkloads sequentially, reducing the need for res ource scheduling", "Run all AI workloads on bare m etal servers without virtualiz ation to maximize performance"], "correct": [0], "multi": false, "required": 1}, {"qnum": 172, "prompt": "You are leading a project to implement a real-time fraud detect ion system for a financial institution. \nThe system needs to analyze tran sactions in real-time using a d eep learning model that has been \ntrained on large datasets. The infe rence workload must be highl y scalable and capable of processing \nthousands of transactions per second with minimal latency. Your  deployment environment includes \nNVIDIA A100 GPUs in a Kubernetes-m anaged cluster. Which approac h would be most suitable to \ndeploy and manage your deep lear ning inference workload?", "options": ["NVIDIA TensorRT Standalone", "NVIDIA Triton Inferen ce Server with Kubernetes", "Apache Kafka with NVIDIA GPUs", "NVIDIA CUDA Toolkit with Docker"], "correct": [1], "multi": false, "required": 1}, {"qnum": 173, "prompt": "Which component of the NVIDIA AI software st ack is primarily responsible for optimizing deep \nlearning inference performance by leveraging the specific archi tecture of NVIDIA GPUs?", "options": ["NVIDIA cuDNN", "NVIDIA TensorRT", "NVIDIA Triton Inference Server", "NVIDIA CUDA Toolkit"], "correct": [1], "multi": false, "required": 1}, {"qnum": 174, "prompt": "You are working on a project that in volves both real-time AI in ference and data preprocessing tasks. \nThe AI models require high thr oughput and low latency, while th e data preprocessing involves \ncomplex logic and diverse data t ypes. Given the need to balance  these tasks, which computing \narchitecture should you prioritize for each task?", "options": ["Use GPUs for both AI infe rence and data preprocessing", "Use CPUs for both AI infe rence and data preprocessing", "Prioritize GPUs for AI infe rence and CPUs for data preproces sing", "Deploy AI inference on CPUs and data preprocessing on FPGAs"], "correct": [2], "multi": false, "required": 1}, {"qnum": 175, "prompt": "You are responsible for managi ng an AI infrastructure that incl udes multiple GPU clusters for deep \nlearning workloads. One of your tasks is to efficiently allocat e resources and manage workloads \nacross these clusters using an or chestration platform. Which of  the following approaches would best \noptimize the utilization of GPU resources while ensuring high availability of the AI workloads?", "options": ["Use a round-robin scheduling algorithm across all GPU cluste rs", "Assign workloads to clusters  based on a predefined static sc hedule", "Implement a load-balancing algorithm that dynamically assigns workloads based on real-time GPU \navailability", "Use a first-come, first-served (FCFS) scheduling policy across all clusters"], "correct": [2], "multi": false, "required": 1}, {"qnum": 176, "prompt": "Your organization has deployed a l arge-scale AI data center with multiple GPUs running complex \ndeep learning workloads. You've  noticed fluctuating performance and increasing energy \nconsumption across several nodes. You need to optimize the data  center's operation and improve \nenergy efficiency while ensuri ng high performance. Which of the  following actions should you \nprioritize to achieve optimized A I data center management and m aintain efficient \nenergyconsumption?", "options": ["Disable power management features on all GPUs to ensure maxi mum performance", "Implement GPU workload sched uling based on real-time perform ance metrics", "Install additional GPUs to distribute the workload more evenly", "Increase the number of activ e cooling systems to reduce thermal throttling"], "correct": [1], "multi": false, "required": 1}, {"qnum": 177, "prompt": "You are working with a large data set containing millions of rec ords related to cus tomer behavior. \nYour goal is to identify key tre nds and patterns that could imp rove your company's product \nrecommendations. You have access to a high-performance AI infra structure with NVIDIA GPUs, and \nyou want to leverage this for e fficient data mining. Which tech nique would most effectively utilize \nthe GPUs to extract actionable insights from the dataset?", "options": ["Visualizing the data using a s tandard spreadsheet applicatio n", "Using traditional SQL queries to filter and sort the data", "Implementing deep learning models for clustering customers i nto segments", "Employing a simple decision tre e model to classify customer data"], "correct": [2], "multi": false, "required": 1}, {"qnum": 178, "prompt": "Your AI team is running a distributed deep learn ing training jo b on an NVIDIA DGX A100 clusterusing \nmultiple nodes. The training process is slowing down significan tly as the model size increases. Which \nof the following strategies woul d be most effective in optimizi ng the training performance?", "options": ["Enable Mixed Precision Training", "Use Data Parallelism In stead of Model Parallelism", "Decrease the Number of Nodes", "Increase Batch Size"], "correct": [0], "multi": false, "required": 1}, {"qnum": 179, "prompt": "A financial services company is  using an AI model for fraud detection, deployed on NVIDIA GPUs. \nAfter deployment, the company no tices a significant delay in processing transactions, which impacts \ntheir operations. Upon investigation, it's discovered that the AI model is being heavily used during peak business hours, leading to r esource contention on the GPUs. What is the best approach to \naddress this issue?", "options": ["Switch to using CPU resources instead of GPUs for processing", "Disable GPU monito ring to free up resources", "Increase the batch size of  input data for the AI model", "Implement GPU load balancing across multiple instances"], "correct": [3], "multi": false, "required": 1}, {"qnum": 180, "prompt": "Your organization is setting up a n AI infrastructure to support  a range of AI workloads, including data \nprocessing, model training, and inf erence. The infrastructure needs to be scalable, support \ndistributed training, and handle large datasets efficiently. Wh ich NVIDIA solution would be most \nsuitable for managing and orchestr ating this AI infrastructure?", "options": ["NVIDIA DeepOps", "NVIDIA TensorRT", "NVIDIA RAPIDS", "NVIDIA DGX Systems"], "correct": [0], "multi": false, "required": 1}, {"qnum": 181, "prompt": "Your AI infrastructure team is  observing out-of-memory (OOM) er rors during the execution of large \ndeep learning models on NVIDIA GPUs. To prevent these errors an d optimize model performance, \nwhich GPU monitoring metric is most critical?", "options": ["GPU Memory Usage", "GPU Core Utilization", "Power Usage", "PCIe Bandwidth Utilization"], "correct": [0], "multi": false, "required": 1}, {"qnum": 182, "prompt": "An organization is deploying a l arge-scale AI model across mult iple NVIDIA GPUs in a data center. The \nmodel training requires extensive GPU-to-GPU communication to exchange gradients. Which of the following networking technologies is most appropriate for minim izing communication latency and \nmaximizing bandwidth between GPUs?", "options": ["InfiniBand", "Ethernet", "Wi-Fi", "Fibre Channel"], "correct": [0], "multi": false, "required": 1}, {"qnum": 183, "prompt": "You are managing an AI infrastructure where multiple AI workloa ds are being run in parallel, \nincluding image recognition, natu ral language processing (NLP),  and reinforcement learning. Due to \nlimited resources, you need to priori tize these workloads. Which AI workload should you prioritize \nfirst to ensure the best overall system performance and resourc e allocation?", "options": ["Image recognition", "Reinforcement learning", "Natural Language Processing (NLP)", "Background data preprocessing"], "correct": [2], "multi": false, "required": 1}, {"qnum": 184, "prompt": "You are deploying an AI model on a c loud-based infrastructure u sing NVIDIA GPUs. During the \ndeployment, you notice that the model's inference times vary si gnificantly across different instances, \ndespite using the same instance t ype. What is the most likely c ause of this inconsistency?", "options": ["Differences in the versions of  the CUDA toolkit installed on  the instances", "The model architecture is no t suitable for GPU acceleration", "Network latency between cloud regions", "Variability in the GPU load due to other tenants on the same  physical hardware"], "correct": [3], "multi": false, "required": 1}, {"qnum": 185, "prompt": "In managing an AI data center , you need to ensure continuous optimal performance and quickly \nrespond to any potential issues. Whi ch monitoring tool or appro ach would best suit the need to \nmonitor GPU health, usage, and performance metrics across all d eployed AI workloads?", "options": ["Nagios Monitoring System", "Prometheus with Node Exporter", "Splunk", "NVIDIA DCGM (Data Center GPU Manager)"], "correct": [3], "multi": false, "required": 1}, {"qnum": 186, "prompt": "In an AI data center, you are wor king with a professional admin istrator to optimize the deployment \nof AI workloads across multiple s ervers. Which of the following actions would best contribute to \nimproving the efficiency and performance of the data center?", "options": ["Distribute AI workloads across multiple servers with GPUs, w hile using DPUs to manage network \nand storage tasks", "Consolidate all AI workloads onto a single high-performance server to maximize GPU utilization", "Allocate all networking tasks  to the CPUs, allowing the GPUs  and DPUs to focus solely on AI model \ncomputation", "[Note: Original question only provided three options; assumi ng a typo and treating A as the \nintended correct answer]"], "correct": [0], "multi": false, "required": 1}, {"qnum": 187, "prompt": "Your company is implementing a hybr id cloud AI inf rastructure t hat needs to support both onpremises \nand cloud-based AI workloads. The  infrastructure must enable se amless integration, \nscalability, and efficient resource management across different environments. Which NVIDIA solution \nshould be considered to best s upport this hybrid infrastructure ?", "options": ["NVIDIA MIG (Multi-Instance GPU)", "NVIDIA Triton Inference Server", "NVIDIA Clara Deploy SDK", "NVIDIA Fleet Command"], "correct": [3], "multi": false, "required": 1}, {"qnum": 188, "prompt": "Which networking feature is most important for supporting distr ibuted training of large AI models \nacross multiple data centers?", "options": ["High throughput with low latency  WAN links between data centers", "Implementation of Quality of S ervice (QoS) policies to prior itize AI training traffic", "Segregated network segments t o prevent data leakage between AI tasks", "Deployment of wireless netw orking to enable flexible node pl acement"], "correct": [0], "multi": false, "required": 1}, {"qnum": 189, "prompt": "Your AI training jobs are consiste ntly taking longer than expec ted to complete on your GPU cluster, \ndespite having optimized your mode l and code. Upon investigatio n, you notice that some GPUs are \nsignificantly underutilized. What could be the most likely caus e of this issue?", "options": ["Insufficient power supply to the GPUs", "Inefficient data pipeline causing bottlenecks", "Inadequate cooling lead ing to thermal throttling", "Outdated GPU drivers"], "correct": [1], "multi": false, "required": 1}, {"qnum": 190, "prompt": "In a virtualized AI environmen t, you are responsible for managing GPU resources across several VMs \nrunning different AI workloads. W hich approach would most effec tively allocate GPU resources to \nmaximize performance and flexibility?", "options": ["Deploy all AI workloads in a single VM with multiple GPUs to  centralize resource management", "Assign a dedicated GPU to each V M to ensure consistent perfo rmance for each AI workload", "Implement GPU virtualization to allow multiple VMs to share GPU resources dynamically based on \ndemand", "Use GPU passthrough to allocat e full GPU resources directly to one VM at a time, based on the \nhighest priority workload"], "correct": [2], "multi": false, "required": 1}, {"qnum": 191, "prompt": "You are assisting a professional administrator in ensuring data integrity during AI model training in an \nAI data center. Which of the fo llowing strategies would best co ntribute to maintaining data integrity \nacross distributed GPU nodes?", "options": ["Use a single master node with GPUs to manage all data proces sing and then distribute the results \nto other nodes", "Assign data verification tasks to DPUs, allowing GPUs to foc us solely on model training", "Utilize redundant GPU nodes to independently process data an d compare results post-training", "Implement a distributed file s ystem with replication, ensuri ng that each GPU node has access to \nthe same consistent dataset"], "correct": [3], "multi": false, "required": 1}, {"qnum": 192, "prompt": "In an effort to improve energy efficiency in your AI infrastruc ture using NVIDIA GPUs, you're \nconsidering several strategies . Which of the following would mo st effectively balance energy \nefficiency with maintaining performance?", "options": ["Enabling deep sleep mode on a ll GPUs during processing times", "Disabling all energy-saving features to ensure maximum performance", "Running all GPUs at the lowe st possible clock speeds", "Employing NVIDIA GPU Boos t technology to dynamically adjust clock speeds"], "correct": [3], "multi": false, "required": 1}, {"qnum": 193, "prompt": "A large manufacturing company is implementing an AI-based predi ctive maintenance system to \nreduce downtime and increase the efficiency of its production l ines. The AI system must analyze data \nfrom thousands of sensors in real-time to predict equipment fai lures before they occur. However, \nduring initial testing, the system  fails to process the incomin g data quickly enough, leading to \ndelayed predictions and occasional missed failures. What would be the most effective strategy to \nenhance the system's real-time processing capabilities?", "options": ["Reduce the number of sensors  to decrease the amount of data the AI system must process", "Use a more complex AI model to enhance prediction accuracy", "Implement edge computing to pre process sensor data closer to  the source before sending it to the \ncentral AI system", "Increase the frequency of sen sor data collection to provide more detailed inputs for the AI model"], "correct": [2], "multi": false, "required": 1}, {"qnum": 194, "prompt": "Which of the following is a prim ary challenge when integrating AI into existing IT  infrastructure?", "options": ["Ensuring AI models have a user-friendly interface", "Scalability of the AI workloads", "Finding AI tools that are com patible with existing hardware", "Selecting the right cloud service provider"], "correct": [1], "multi": false, "required": 1}, {"qnum": 195, "prompt": "A retail company is consideri ng using AI to enhance its operati ons. They want to improve customer \nexperience, optimize inventory management, and personalize marketing campaigns. Which AI use case would be most impactfu l in achieving these goals?", "options": ["AI-powered recommendation sys tems, which personalize product  suggestions for customers based on their behavior", "Natural language processing for  automated customer support chatbots", "AI-driven fraud detection to pr event unauthorized transactio ns", "Image recognition for automa tic labeling of products in ware houses"], "correct": [0], "multi": false, "required": 1}, {"qnum": 196, "prompt": "Which two software components ar e directly involv ed in the life cycle of AI development and \ndeployment, particularly in mode l training and model serving? ( Select two)", "options": ["Prometheus", "MLflow", "Airflow", "Apache Spark", "Kubeflow"], "correct": [1, 4], "multi": true, "required": 2}, {"qnum": 197, "prompt": "Which of the following statements  correctly highlights a key di fference between GPU a nd CPU architectures?", "options": ["CPUs are optimized for parallel processing, making them bett er for AI workloads, while GPUs are designed for sequential tas ks", "GPUs typically have higher clock speeds than CPUs, allowing them to process individual tasks faster", "CPUs are specialized for gra phical computations, whereas GPU s handle general-purpose computing", "GPUs are optimized for para llel processing, with thousands of smaller cores, while CPUs have fewer, more powerful cores for sequential tasks"], "correct": [3], "multi": false, "required": 1}]};
let state = { mode:'menu', reviewRound:0, order:[], currentIndex:0, startTime:0, timerId:null, wrongInThisRun:[], reviewPool:[], reviewRandom:false, doubleLock:false };
const view = document.getElementById('view');
const timerEl = document.getElementById('timer');
const progressBar = document.getElementById('progressBar');
const progressText = document.getElementById('progressText');
const homeBtn = document.getElementById('homeBtn');
const restartBtn = document.getElementById('restartBtn');
const modal = document.getElementById('modal');
const closeModal = document.getElementById('closeModal');
function fmtTime(ms){ const s=Math.floor(ms/1000); const mm=String(Math.floor(s/60)).padStart(2,'0'); const ss=String(s%60).padStart(2,'0'); return `${mm}:${ss}`; }
function startTimer(){ state.startTime=Date.now(); if(state.timerId) clearInterval(state.timerId); state.timerId=setInterval(()=>{ const e=Date.now()-state.startTime; timerEl.textContent='‚è± '+fmtTime(e); },250); }
function stopTimer(){ if(state.timerId) clearInterval(state.timerId); state.timerId=null; }
function shuffle(a){ a=a.slice(); for(let i=a.length-1;i>0;i--){ const j=Math.floor(Math.random()*(i+1)); [a[i],a[j]]=[a[j],a[i]]; } return a; }
function resetCommon(){ state.currentIndex=0; state.wrongInThisRun=[]; startTimer(); updateProgress(); }
function updateProgress(){ const total=state.order.length||0; const cur=Math.min(state.currentIndex+1, total); progressText.textContent = total? `${cur}/${total}` : ''; const pct= total? (cur/total*100):0; progressBar.style.width=pct+'%'; }
function showMenu(){ stopTimer(); timerEl.textContent='‚è± 00:00'; progressText.textContent=''; progressBar.style.width='0%'; state.mode='menu'; view.innerHTML=`<div class="menu"><div class="title">Bem-vindo!</div><div class="subtitle">Escolha um modo e bons estudos.</div><button class="btn primary" id="startLearn">üìò Modo Aprendizagem</button><button class="btn primary" id="startLearnRand">üîÄ Modo Aprendizagem Aleat√≥rio</button><button class="btn success" id="startHard">üî• Modo Dif√≠cil</button><button class="btn" id="openHelp">‚ùì Instru√ß√µes</button></div>`; document.getElementById('startLearn').onclick=()=> startLearn(false); document.getElementById('startLearnRand').onclick=()=> startLearn(true); document.getElementById('startHard').onclick=()=> startHard(); document.getElementById('openHelp').onclick=()=> modal.classList.add('show'); }
closeModal.onclick = ()=> modal.classList.remove('show');
homeBtn.onclick = ()=>{ showMenu(); };
restartBtn.onclick = ()=>{
  if(state.mode==='learn') startLearn(false);
  else if(state.mode==='learn-rand') startLearn(true);
  else if(state.mode==='hard') startHard();
  else if(state.mode==='review') startReview(state.reviewPool, state.reviewRandom, state.reviewRound);
};
function startLearn(random){ state.mode = random? 'learn-rand':'learn'; const idx=DATA.questions.map((_,i)=>i); state.order = random? shuffle(idx):idx; state.reviewRound = 0; state.reviewRandom = random; state.reviewPool=[]; resetCommon(); renderQuestion(); }
function startHard(){ state.mode='hard'; const idx=DATA.questions.map((_,i)=>i); state.order = shuffle(idx); state.reviewRound=0; state.reviewPool=[]; resetCommon(); renderQuestion(); }
function startReview(poolIdx, random, round){ state.mode='review'; state.reviewRound = round || 1; state.reviewRandom = !!random; state.reviewPool = poolIdx.slice(); let order = poolIdx.slice(); if(random) order = shuffle(order); state.order = order; resetCommon(); renderQuestion(); }
function buildOptionUI(optText, idx, type){ const id = 'opt_' + idx; const input = type==='multi' ? `<input type="checkbox" id="${id}"/>` : `<input type="radio" name="opts" id="${id}"/>`; return `<label class="option" data-idx="${idx}">${input}<span>${htmlEscape(optText)}</span></label>`; }
function htmlEscape(s){ const div = document.createElement('div'); div.textContent = s; return div.innerHTML; }
function renderQuestion(){ updateProgress(); const qi = state.order[state.currentIndex]; const q = DATA.questions[qi]; const isMulti = q.required && q.required>1; const original = q.options.map((t,i)=>({text:t, idx:i})); const shuffled = shuffle(original); const newCorrectIdx = new Set(q.correct.map(ci=> shuffled.findIndex(o=>o.idx===ci))); const qnum = q.qnum? `Q${q.qnum}` : ''; let html = `<div class="title"><span class="qnum">${qnum}</span> ‚Äî ${htmlEscape(q.prompt)}</div>`; html += `<div class="subtitle">${isMulti? 'Selecione '+q.required+' e clique em Confirmar' : 'Clique em uma alternativa'}</div>`; html += `<div id="options">`; const type = isMulti? 'multi':'single'; shuffled.forEach((opt, i)=>{ html += buildOptionUI(opt.text, i, type); }); html += `</div>`; html += `<div class="footer-actions">`; if(isMulti){ html += `<button class="btn primary" id="confirmBtn">Confirmar</button>`; } if(state.mode!=='hard'){ html += `<button class="btn" id="skipBtn" style="display:none">Pr√≥xima pergunta ‚ñ∂</button>`; } html += `</div>`; view.innerHTML = html; const optionEls = Array.from(document.querySelectorAll('.option')); const disableAll = (v)=> optionEls.forEach(el=> el.style.pointerEvents = v? 'none':'auto'); const markResult = (correctSet, chosenSet)=>{ optionEls.forEach((el, i)=>{ if(correctSet.has(i)) el.classList.add('correct'); if(chosenSet.has(i) && !correctSet.has(i)) el.classList.add('incorrect'); }); }; const nextBtn = document.getElementById('skipBtn'); function goNext(){ state.currentIndex++; if(state.currentIndex >= state.order.length){ return finishRun(); } renderQuestion(); }
  if(!isMulti){ optionEls.forEach(el=>{ el.addEventListener('click', ()=>{ if(state.doubleLock) return; state.doubleLock=true; setTimeout(()=>state.doubleLock=false, 300); const idx = Number(el.getAttribute('data-idx')); const correctSet = new Set(newCorrectIdx); const chosen = new Set([idx]); const ok = correctSet.has(idx); markResult(correctSet, chosen); disableAll(true); if(state.mode!=='hard'){ nextBtn.style.display='inline-block'; nextBtn.onclick=()=> goNext(); if(!ok){ state.wrongInThisRun.push(qi); } } else { if(ok){ setTimeout(()=> goNext(), 400); } else { showHardFailMenu(q, correctSet); } } }); }); } else { const confirmBtn = document.getElementById('confirmBtn'); confirmBtn.addEventListener('click', ()=>{ if(state.doubleLock) return; state.doubleLock=true; setTimeout(()=>state.doubleLock=false, 300); const chosen = new Set(); optionEls.forEach((el,i)=>{ const input = el.querySelector('input'); if(input && input.checked) chosen.add(i); }); const correctSet = new Set(newCorrectIdx); const ok = chosen.size===correctSet.size && Array.from(chosen).every(i=>correctSet.has(i)); markResult(correctSet, chosen); optionEls.forEach(el=> el.style.pointerEvents='none'); if(state.mode!=='hard'){ nextBtn.style.display='inline-block'; nextBtn.onclick=()=> goNext(); if(!ok){ state.wrongInThisRun.push(qi); } } else { if(ok){ setTimeout(()=> goNext(), 400); } else { showHardFailMenu(q, correctSet); } } }); }
}
function showHardFailMenu(q, correctSet){ const correctLetters = Array.from(correctSet).map(i=> String.fromCharCode(65+i)).join(', '); view.innerHTML += `<div class="notice"><b>RESPOSTA INCORRETA.</b> A correta √©: <span class="correct-text">${correctLetters}</span></div>`; const div = document.createElement('div'); div.className='footer-actions'; const tryBtn = document.createElement('button'); tryBtn.className='btn danger'; tryBtn.textContent='Tentar novamente'; const goLearnBtn = document.createElement('button'); goLearnBtn.className='btn'; goLearnBtn.textContent='Ir para Modo Aprendizagem'; div.appendChild(tryBtn); div.appendChild(goLearnBtn); view.appendChild(div); tryBtn.onclick=()=> startHard(); goLearnBtn.onclick=()=> startLearn(false); }
function finishRun(){ stopTimer(); const elapsed = Date.now() - state.startTime; const total = state.order.length; const wrongSet = new Set(state.wrongInThisRun); const correctCount = total - wrongSet.size; let summary = `<div class="title">Resultado</div>`; summary += `<div class="kpis">`; summary += `<div class="kpi">Acertos: <b>${correctCount}</b> / ${total}</div>`; const pct = Math.round(correctCount/total*100); summary += `<div class="kpi">Percentual: <b>${pct}%</b></div>`; summary += `<div class="kpi">Tempo: <b>${fmtTime(elapsed)}</b></div>`; summary += `</div>`; const inHard = state.mode==='hard'; const inReview = state.mode==='review'; if((inHard && wrongSet.size===0) || (inReview && wrongSet.size===0)){ fireConfetti(); } if(!(state.mode==='hard' && wrongSet.size===0)){ const wrongList = state.order.filter(idx=> wrongSet.has(idx)); let details = ''; if(wrongList.length){ details += '<details class="collapsible"><summary>Resumo de tentativas ‚Äî quest√µes incorretas</summary><div class="panel">'; wrongList.forEach((qi)=>{ const q = DATA.questions[qi]; const qnum = q.qnum? 'Q'+q.qnum : ''; details += `<div class="small"><b>${qnum}</b> ‚Äî ${htmlEscape(q.prompt)}</div>`; }); details += '</div></details>'; } summary += details; }
  const footer = document.createElement('div'); footer.className='footer-actions'; if(state.mode==='learn' || state.mode==='learn-rand' || state.mode==='review'){ const wrongList = state.order.filter(idx=> wrongSet.has(idx)); if(wrongList.length){ const revBtn = document.createElement('button'); revBtn.className='btn primary'; revBtn.textContent = (state.mode==='learn-rand' || (state.mode==='review' && state.reviewRandom)) ? '‚ñ∂ Iniciar Revis√£o (aleat√≥ria)' : '‚ñ∂ Iniciar Revis√£o'; revBtn.onclick = ()=>{ const nextRound = state.mode==='review'? state.reviewRound+1 : 1; const random = (state.mode==='learn-rand' || (state.mode==='review' && state.reviewRandom)); startReview(wrongList, random, nextRound); }; footer.appendChild(revBtn); } const againBtn = document.createElement('button'); againBtn.className='btn'; againBtn.textContent='Refazer este modo'; againBtn.onclick = ()=>{ if(state.mode==='learn') startLearn(false); else if(state.mode==='learn-rand') startLearn(true); else if(state.mode==='review') startReview(state.reviewPool, state.reviewRandom, state.reviewRound); }; footer.appendChild(againBtn); }
  if(state.mode==='hard'){ if(wrongSet.size===0){ const okBtn = document.createElement('button'); okBtn.className='btn success'; okBtn.textContent='üéâ PARAB√âNS! Voltar ao menu'; okBtn.onclick = ()=> showMenu(); footer.appendChild(okBtn); } else { const tryBtn = document.createElement('button'); tryBtn.className='btn danger'; tryBtn.textContent='Tentar novamente (novo embaralhamento)'; tryBtn.onclick = ()=> startHard(); footer.appendChild(tryBtn); const goLearnBtn = document.createElement('button'); goLearnBtn.className='btn'; goLearnBtn.textContent='Ir para Modo Aprendizagem'; goLearnBtn.onclick = ()=> startLearn(false); footer.appendChild(goLearnBtn); } }
  const home = document.createElement('button'); home.className='btn'; home.textContent='üè† Home'; home.onclick=()=> showMenu(); footer.appendChild(home);
  view.innerHTML = `<div class="title">${(state.mode==='hard' && wrongSet.size===0) ? 'PARAB√âNS!' : 'Resultado'}</div>` + summary; view.appendChild(footer);
}
function fireConfetti(){ const c = document.getElementById('confetti'); c.innerHTML=''; for(let i=0;i<120;i++){ const s = document.createElement('div'); s.textContent = ['üéâ','‚ú®','üü¶','üü£','üü©','üü°'][i%6]; s.style.position='absolute'; s.style.left = Math.random()*100 + 'vw'; s.style.top = '-10px'; s.style.fontSize = (12+Math.random()*18) + 'px'; s.style.transition = 'transform 1.5s ease, opacity 1.8s'; c.appendChild(s); setTimeout(()=>{ s.style.transform = `translateY(${90+Math.random()*10}vh) rotate(${(Math.random()*720-360)}deg)`; s.style.opacity = '0'; }, 10+i*4); } setTimeout(()=> c.innerHTML='', 2200); }
// Keyboard shortcuts: Enter/Space -> Confirmar ou Pr√≥xima
 document.addEventListener('keydown', (e)=>{
   if(e.key==='Enter' || e.key===' '){
     const confirmBtn=document.getElementById('confirmBtn');
     const nextBtn=document.getElementById('skipBtn');
     const active=document.activeElement; const isTyping = active && (active.tagName==='INPUT' || active.tagName==='TEXTAREA');
     if(isTyping) return;
     if(confirmBtn && confirmBtn.offsetParent!==null){ e.preventDefault(); confirmBtn.click(); return; }
     if(nextBtn && nextBtn.offsetParent!==null){ e.preventDefault(); nextBtn.click(); return; }
   }
 });
showMenu();
</script>
</body>
</html>
